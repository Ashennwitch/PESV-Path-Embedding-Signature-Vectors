%-----------------------------------------------------------------------------%
\chapter{\babTiga}
\label{cha:perancangansistem}

\section{Pemilihan dan Pra-Pemrosesan Dataset ISCX 2016}

\subsection{Deskripsi Dataset ISCX 2016}
Dataset ISCXVPN2016 merupakan dataset yang paling populer dan banyak digunakan dalam penelitian klasifikasi lalu lintas jaringan terenkripsi, khususnya untuk membedakan trafik VPN dan Non-VPN \citep{DraperGil2016, razooqi2025vpn}. Dataset ini berisi trafik nyata dari berbagai aplikasi populer yang dikategorikan ke dalam tiga tugas klasifikasi utama: enkapsulasi (encapsulation), kategori aplikasi, dan aplikasi spesifik. Tugas enkapsulasi membedakan antara trafik VPN dan Non-VPN, kategori aplikasi mengelompokkan trafik berdasarkan jenis layanan seperti chat, email, streaming, transfer file, peer-to-peer, dan VoIP, sedangkan tugas aplikasi menentukan aplikasi spesifik yang menghasilkan trafik tersebut dengan total 16 kelas aplikasi \citep{park2024fast}.

Tabel \ref{tab:dataset_distribution} menunjukkan distribusi kelas pada ketiga tugas klasifikasi dalam dataset ISCXVPN2016.

\begin{table}[h]
    \centering
    \caption{Distribusi Kelas pada Dataset ISCXVPN2016 \citep{park2024fast}}
    \label{tab:dataset_distribution}
    \setlength{\tabcolsep}{6pt} % mengurangi jarak antar kolom
    \renewcommand{\arraystretch}{1.2} % sedikit longgar untuk keterbacaan
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        \textbf{Tugas} & \textbf{Kelas} \\
        \midrule
        Enkapsulasi (2 kelas) & VPN, Non-VPN \\
        Kategori (6 kelas) & Chat, Email, Streaming, File Transfer, P2P, VoIP \\
        Aplikasi (16 kelas) & Skype, ICQ, Hangout, Facebook, Email, Gmail, FTP, SCP, SFTP, Netflix, Spotify, Vimeo, YouTube, AIM Chat, VOIPBuster, BitTorrent \\
        \bottomrule
    \end{tabularx}
\end{table}

Dengan tiga tingkat klasifikasi ini, dataset ISCXVPN2016 menyediakan kerangka kerja evaluasi yang komprehensif dan representatif untuk penelitian network traffic classification, terutama dalam konteks trafik terenkripsi yang semakin berkembang.

\subsection{Pra-Pemrosesan Dataset ISCX 2016}
\label{subsec:prapemrosesan}

Tujuan dari pra-pemrosesan dataset ISCX 2016 adalah untuk mengubah data mentah \texttt{.pcap}, yang berisi data level-paket, menjadi format \textit{flow-level} yang terstruktur dan bersih. Proses ini penting karena dataset aslinya, yang berisi sekitar 310.000 \textit{flow}, mengandung banyak trafik yang tidak relevan atau \textit{noise} yang dihasilkan oleh sistem operasi atau konfigurasi jaringan (seperti DNS, NBSS, LLMNR), bukan oleh aplikasi yang menjadi target analisis.

Proses ini didasarkan pada metodologi yang diuraikan oleh \citep{park2024fast}, yang mengadopsi dan memperluas proses pembersihan dua tahap dari \citep{baek2023preprocessing}. Pra-pemrosesan ini terdiri dari tiga langkah utama:

\medskip
\noindent\textbf{1. Konversi Paket-ke-Flow (menggunakan SplitCap)} \\
Langkah pertama adalah mengubah file \texttt{.pcap} mentah menjadi \textit{bidirectional flows} (sesi). Konversi ini krusial karena analisis level \textit{flow} (konversasi utuh) memberikan konteks yang jauh lebih kaya daripada analisis level paket (kata-kata individual). Analisis \textit{flow} juga memungkinkan ekstraksi fitur statistik yang lebih bermakna (misalnya, durasi \textit{flow}, total byte, waktu antar-paket) yang esensial untuk membedakan trafik terenkripsi tanpa perlu menginspeksi \textit{payload}.

Untuk tugas ini, \textit{tool} SplitCap digunakan. SplitCap dipilih karena merupakan \textit{tool} khusus yang teroptimasi (ditulis dalam C/C++) untuk menangani file \texttt{.pcap} besar secara efisien dan akurat dalam mengelola sesi, yang esensial untuk mereproduksi metodologi penelitian.

Untuk mengotomatisasi pemrosesan pada seluruh dataset, sebuah skrip Python digunakan untuk memanggil SplitCap pada setiap file \texttt{.pcap}, seperti yang ditunjukkan pada Listing~\ref{lst:splitcap_automation}. Perlu dicatat penambahan argumen \texttt{-p 500} yang penting untuk membatasi sesi paralel dan menghindari galat ``Too many open files''.

\begin{listing}[H]
\setstretch{1}
\begin{minted}[fontsize=\footnotesize, linenos]{python}
def run_splitcap(pcap_file_path, output_dir):
    """Constructs and runs the SplitCap command for a single pcap file.
    Splits the file into bidirectional flows (sessions).

    UPDATED: Includes the -p argument to limit parallel file handles.
    """
    # From the documentation, the argument for bidirectional flows is 'session'
    split_argument = 'session'

    # Set a limit for parallel sessions to prevent "Too many open files" error
    parallel_sessions_limit = '500'

    # The command to execute in WSL
    command = [
        'mono',
        SPLITCAP_EXECUTABLE,
        '-r', pcap_file_path,
        '-o', output_dir,
        '-s', split_argument,
        '-p', parallel_sessions_limit  # <-- THIS IS THE NEW ARGUMENT
    ]

    print(f"-> Processing: {pcap_file_path}")
    print(f"   Command: {' '.join(command)}")

    try:
        # Execute the command
        result = subprocess.run(
            command,
            check=True,       # Raises exception if command fails
            capture_output=True,
            text=True
        )
        print(f"   Success! Output saved to: {output_dir}")

    except FileNotFoundError:
        print("\n [ERROR] 'mono' command not found.")
        print("Please ensure Mono is installed...")
        return False

    except subprocess.CalledProcessError as e:
        print(f"\n [ERROR] SplitCap failed for file: {pcap_file_path}")
        print(f"   Return Code: {e.returncode}")
        print(f"   Error Output:\n{e.stderr}")
        return False

    return True
\end{minted}
\captionsetup{font=small}
\caption{Skrip Python untuk otomatisasi konversi \texttt{.pcap} ke \textit{flow} menggunakan SplitCap.}
\label{lst:splitcap_automation}
\end{listing}

\medskip
\noindent\textbf{2. Pembersihan Awal (Rule A \& B)} \\
Setelah semua \texttt{.pcap} dikonversi menjadi \textit{flow} (total $\sim$310.000), proses pembersihan dua tahap yang diadopsi dari \citep{baek2023preprocessing} diterapkan:
\begin{enumerate}
    \item \textbf{Penghapusan Flow Berbasis Protokol (Rule A):} \textit{Flow} yang menggunakan protokol yang tidak relevan dengan aplikasi pengguna, seperti DNS, NBSS, dan LLMNR, dihapus.
    \item \textbf{Verifikasi TCP 3-Way Handshake (Rule B):} \textit{Flow} TCP yang tidak memiliki 3-way handshake (SYN, SYN-ACK, ACK) lengkap di awal sesi akan dibuang. Ini memastikan bahwa hanya sesi TCP yang valid dan lengkap yang disimpan. Implementasi spesifiknya adalah menghapus \textit{flow} TCP dengan lebih dari 4 paket di mana paket pertamanya tidak memiliki \textit{flag} SYN.
\end{enumerate}

Setelah penerapan kedua aturan ini, jumlah \textit{flow} berkurang secara signifikan menjadi \textbf{29.195 \textit{flow}}.

\medskip
\noindent\textbf{3. Penyaringan Tambahan (Rule C)} \\
Langkah terakhir adalah penyaringan tambahan yang diimplementasikan oleh \citep{park2024fast} untuk menghilangkan \textit{flow} yang dianggap tidak penting untuk tujuan penelitian. Penyaringan ini (disebut Rule C) secara spesifik mencari dan menghapus \textit{flow} yang memenuhi \textit{semua} kriteria berikut:
\begin{itemize}
    \item Protokol adalah \textbf{UDP}.
    \item Alamat IP tujuan adalah \textbf{\texttt{255.255.255.255}} (broadcast).
    \item \textit{Payload} spam berisi string \textbf{\texttt{"Beacon\textasciitilde"}} \citep{park2024fast}.
\end{itemize}

\medskip
\noindent\textbf{Implementasi Penyaringan (Rule A, B, dan C)} \\
Ketiga aturan penyaringan (A, B, dan C) diimplementasikan dalam satu skrip Python menggunakan Scapy untuk membaca dan menganalisis setiap \textit{flow} \texttt{.pcap} satu per satu. Fungsi inti dari proses validasi ini ditunjukkan pada Listing~\ref{lst:scapy_filtering}.

\begin{listing}[H]
\setstretch{1}
\begin{minted}[fontsize=\footnotesize, linenos]{python}
def is_flow_valid_final(pcap_file_path):
    """Analyzes a single flow pcap file and applies ALL filtering rules
    (A, B, and C) to determine if it belongs in the final dataset.

    Returns a tuple: (is_valid, reason_for_invalidation)
    """
    try:
        packets = rdpcap(pcap_file_path)
    except Exception as e:
        return False, f"Scapy could not read file. Error: {e}"

    if not packets:
        return False, "File is empty or corrupt"

    # --- Implement ALL Filtering Rules ---
    first_packet = packets[0]

    # Rule B: TCP 3-Way Handshake Check
    if TCP in first_packet:
        if len(packets) > 4 and 'S' not in first_packet[TCP].flags:
            return False, "Rule B Failed: Invalid TCP handshake start"

    # Loop through all packets to check Rules A and C
    for pkt in packets:
        # Rule A (Part 1): Disallowed protocols
        for protocol_layer in DISALLOWED_PROTOCOLS:
            if protocol_layer in pkt:
                return False, f"Rule A Failed: Disallowed protocol ({protocol_layer.name})"

        if UDP in pkt:
            # Rule A (Part 2): Disallowed UDP ports
            if pkt[UDP].sport in DISALLOWED_UDP_PORTS or pkt[UDP].dport in DISALLOWED_UDP_PORTS:
                return False, "Rule A Failed: Disallowed UDP port used"

            # Rule C: UDP Beacon Check
            if IP in pkt and pkt[IP].dst == '255.255.255.255':
                try:
                    payload = bytes(pkt[UDP].payload)
                    if b'Beacon~' in payload:
                        return False, "Rule C Failed: UDP Beacon flow detected"
                except Exception:
                    pass

    # If all checks passed, flow is valid
    return True, "Valid"
\end{minted}
\captionsetup{font=small}
\caption{Fungsi validasi \textit{flow} menggunakan Scapy untuk menerapkan Rule A, B, dan C.}
\label{lst:scapy_filtering}
\end{listing}

Setelah ketiga langkah pra-pemrosesan ini selesai, dataset final yang bersih berisi total \textbf{13.487 \textit{flow}}. Dataset yang telah difilter secara ketat inilah yang kemudian digunakan untuk semua proses ekstraksi fitur dan pelatihan model selanjutnya dalam penelitian ini.

\subsection{Pemilihan \textit{Sample} dari \textit{Flow} yang Telah Di-ekstrak}

Setelah proses pra-pemrosesan di bagian \ref{subsec:prapemrosesan} selesai, didapatkan total \textbf{13.487 \textit{flow}} yang bersih. Analisis awal terhadap 13.487 \textit{flow} ini mengungkapkan adanya ketidakseimbangan kelas (\textit{class imbalance}) yang ekstrem pada dataset. Seperti yang ditunjukkan pada Gambar \ref{fig:full_dist_before}, beberapa aplikasi seperti \textit{Skype} memiliki 4.704 sampel, sementara \textit{ICQ} dan \textit{AIM Chat} hanya memiliki kurang dari 50 sampel. Ketidakseimbangan ini dapat menyebabkan model klasifikasi menjadi bias terhadap kelas mayoritas dan memiliki performa buruk pada kelas minoritas.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/binary_dist.png}
        \caption{Tugas Biner (13.487 flow)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/category_dist.png}
        \caption{Tugas Kategori (13.487 flow)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/application_dist.png}
        \caption{Tugas Aplikasi (13.487 flow)}
    \end{subfigure}

    \caption{Distribusi dataset penuh (13.487 flow) sebelum kurasi. Terlihat jelas adanya ketidakseimbangan kelas yang signifikan.}
    \label{fig:full_dist_before}
\end{figure}

Oleh karena itu, dilakukan langkah kurasi dataset untuk menciptakan dataset yang lebih seimbang untuk eksperimen. Dataset yang dikurasi ini (selanjutnya disebut Dataset v2) dirancang untuk memenuhi dua batasan utama:
\begin{enumerate}
    \item Untuk kesederhanaan, dataset akan berisi 6 kelas aplikasi yang paling representatif.
    \item Untuk menjaga komparabilitas dengan penelitian lain \citep{park2024fast}, dataset harus mencakup \textit{semua} 6 kelas kategori (Chat, Email, File Transfer, P2P, Streaming, VoIP).
\end{enumerate}

Tantangannya adalah, jika hanya mengambil 6 aplikasi dengan jumlah sampel terbesar (misalnya \textit{Skype}, \textit{Email}, \textit{SCP}, \textit{VOIPBuster}, \textit{FTP}, \textit{Facebook}), batasan kedua tidak terpenuhi karena tidak ada perwakilan untuk kategori \textit{Streaming} dan \textit{P2P}.

Solusinya adalah melakukan proses \textit{pick-and-choose} dengan memilih 6 aplikasi yang dapat menjadi ``perwakilan'' dengan jumlah sampel terbesar untuk \textit{setiap} kategori. Aplikasi yang terpilih adalah:
\begin{itemize}
    \item \textbf{Skype} (4.704 flow): Perwakilan terbesar untuk Chat, VoIP, dan File Transfer.
    \item \textbf{Email} (1.901 flow): Perwakilan terbesar untuk Email.
    \item \textbf{SCP} (1.741 flow): Perwakilan terbesar kedua untuk File Transfer.
    \item \textbf{VOIPBuster} (1.196 flow): Perwakilan terbesar untuk VoIP-saja.
    \item \textbf{YouTube} (379 flow): Perwakilan terbesar untuk Streaming.
    \item \textbf{BitTorrent} (363 flow): Satu-satunya perwakilan untuk P2P.
\end{itemize}

Proses \textit{pick-and-choose} ini menghasilkan Dataset v2 dengan total \textbf{10.284 \textit{flow}}. Proses penyaringan ini diimplementasikan dengan skrip Python yang membaca 13.487 \textit{flow} dari direktori sumber dan hanya menyalin \textit{flow} yang termasuk dalam 6 aplikasi target ke direktori tujuan. Logika inti dari skrip penyaringan ditunjukkan pada Listing \ref{lst:filter_v2_pseudo}.

\begin{listing}[H]
\setstretch{1}
\begin{minted}[fontsize=\footnotesize, linenos, breaklines]{text}
Dataset v2 Filter (13,487 → 10,284)
-----------------------------------
Input: SRC, DST, KEYWORD_MAP, TARGET_APPS = {Skype, Email, SCP, VOIPBuster, YouTube, BitTorrent}

filter_dataset():
    files = listdir(SRC) or fatal("read error")
    cnt = {copied:0, skipped:0}
    
    for f in files:
        app = next((a for k,(a,_) in KEYWORD_MAP if k in f.lower()), None)
        if app in TARGET_APPS: copy(SRC+f, DST+f); cnt.copied++
        else: cnt.skipped++
    
    Print(f"Copied: {cnt.copied}, Skipped: {cnt.skipped}")
    Print("SUCCESS" if cnt.copied == 10284 else "WARNING: mismatch")

Main:
    if __name__ == "__main__":
        filter_dataset()
\end{minted}
\captionsetup{font=small}
\caption{Pseudocode dari logika inti skrip Python untuk memfilter 13.487 \textit{flow} bersih menjadi Dataset v2 (10.284 \textit{flow}) berdasarkan 6 aplikasi yang telah ditentukan.}
\label{lst:filter_v2_pseudo}
\end{listing}

Setelah proses kurasi ini selesai, distribusi kelas pada Dataset v2 (10.284 \textit{flow}) menjadi lebih seimbang, meskipun masih menunjukkan ketidakseimbangan yang wajar, seperti terlihat pada Gambar \ref{fig:v2_dist_after}. Dataset v2 (VPN/Non-VPN) inilah yang digunakan sebagai \textit{full dataset}, selain \textit{VPN Only} dataset yang akan dijelaskan di bagian berikutnya.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/full-binary_dist.png}
        \caption{Tugas Biner (v2)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/full-category_dist.png}
        \caption{Tugas Kategori (v2)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/full-application_dist.png}
        \caption{Tugas Aplikasi (v2)}
    \end{subfigure}

    \caption{Distribusi Dataset v2 (10.284 flow) setelah proses kurasi.}
    \label{fig:v2_dist_after}
\end{figure}

\subsection{Kurasi Dataset Khusus VPN (VPN-Only)}
Selain Dataset v2 (10.284 \textit{flow}) yang digunakan untuk eksperimen ‘penuh’ (VPN/Non-VPN), penelitian ini juga bertujuan untuk mengevaluasi performa model pada skenario yang lebih spesifik: mengklasifikasikan trafik yang sudah diketahui terenkripsi (VPN).

Tantangan utamanya adalah, jika kita sekadar memfilter Dataset v2 (10.284 \textit{flow}) untuk sampel VPN-nya, dataset yang dihasilkan menjadi tidak representatif dan trivial. Sebagai contoh, aplikasi SCP (1.741 \textit{flow}) akan hilang seluruhnya karena tidak memiliki sampel VPN. Lebih kritis lagi, kategori P2P hanya akan diwakili oleh BitTorrent, dan Email hanya oleh Email. Ini berarti model tidak akan belajar membedakan ‘Chat’ dari ‘P2P’, melainkan hanya membedakan ‘Skype’ dari ‘BitTorrent’, yang merupakan tugas yang jauh lebih mudah.

Untuk mengatasi ini, sebuah dataset ‘Khusus VPN’ yang baru dikurasi langsung dari 13.487 \textit{flow} bersih. Tujuannya adalah untuk menciptakan dataset yang tetap menantang dengan tetap mencakup semua 6 kategori. Dataset ini memiliki batasan yang tidak terhindarkan: kategori P2P dan Email masing-masing hanya akan diwakili oleh BitTorrent (363 \textit{flow} VPN) dan Email (137 \textit{flow} VPN). Oleh karena itu, tantangan sebenarnya dari dataset ini adalah kemampuan model untuk membedakan kategori yang saling tumpang tindih (Chat, VoIP, File Transfer, Streaming) yang diwakili oleh beberapa aplikasi.

Untuk memaksimalkan tumpang tindih ini, 9 aplikasi dipilih:
\begin{itemize}
    \item \textbf{Chat/VoIP/File Transfer (Tumpang Tindih):} Skype (1246 \textit{flow}), Hangout (325 \textit{flow}), Facebook (247 \textit{flow})
    \item \textbf{File Transfer Tambahan:} FTP (101 \textit{flow})
    \item \textbf{Streaming (Variasi):} YouTube (157 \textit{flow}), Spotify (91 \textit{flow}), Netflix (63 \textit{flow})
    \item \textbf{Perwakilan Kategori Wajib:} BitTorrent (363 \textit{flow}), Email (137 \textit{flow})
\end{itemize}

Berbeda dengan Dataset v2 yang disalin secara fisik, dataset ini difilter secara dinamis saat runtime menggunakan skrip klasifikasi utama. Konfigurasi skrip diatur ke \texttt{EXPERIMENT\_MODE = "VPN\_ONLY"} dan \texttt{TARGET\_APPS} diisi dengan 9 aplikasi yang dipilih, seperti ditunjukkan pada Listing \ref{lst:filter_vpnonly_pseudo}.

\begin{listing}[H]
\setstretch{1}
\begin{minted}[fontsize=\footnotesize, linenos, breaklines]{text}
VPN Dataset Filter (9 Apps)
---------------------------

Input: SOURCE_DIR, DEST_DIR, KEYWORD_MAP, TARGET_APPS = {Skype, Hangout, Facebook, BitTorrent, FTP, Email, YouTube, Spotify, Netflix}

filter_and_copy():
    Print(f"Source: {SOURCE_DIR}\nDest: {DEST_DIR}\nFilter: VPN-only")
    start = now()
    
    mkdir(DEST_DIR) or fatal("mkdir failed")
    files = listdir(SOURCE_DIR) or fatal("listdir failed")
    Print(f"{len(files)} files total")
    
    cnt = {proc:0, vpn:0, match:0, copy:0, skip:0}
    
    for f in files:
        if !f.endswith(".pcap"): cnt.skip++; continue
        cnt.proc++
        if cnt.proc % 1000 == 0: Print(f"{cnt.proc} processed")
        
        f_l = f.lower()
        if !f_l.startswith("vpn_"): continue
        cnt.vpn++
        
        app = next((a for k,(a,_) in KEYWORD_MAP if k in f_l), None)
        if app in TARGET_APPS:
            cnt.match++
            if copy(SOURCE_DIR+f, DEST_DIR+f): cnt.copy++
            else: Print(f"Error copying {f}")
    
    Print(f"Finished in {now()-start}s\n{cnt}")

Main:
    if __name__ == "__main__":
        if !GoogleDrive.mounted(): Print("Mount Drive first")
        else: filter_and_copy()
\end{minted}
\captionsetup{font=small}
\caption{Pseudocode dari skrip Python untuk memfilter dataset VPN-only menjadi 9 aplikasi representatif, mencakup semua 6 kategori aplikasi utama.}
\label{lst:filter_vpnonly_pseudo}
\end{listing}


Proses penyaringan dinamis ini menghasilkan dataset final ‘Khusus VPN’ yang terdiri dari 2.730 \textit{flow} VPN. Distribusi kelas untuk dataset ini ditunjukkan pada Gambar \ref{fig:vpn_only_dist}.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
\begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/vpnonly-category_dist.png}
        \caption{Tugas Kategori (Khusus VPN)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/vpnonly-application_dist.png}
        \caption{Tugas Aplikasi (Khusus VPN)}
    \end{subfigure}

    \caption{Distribusi Dataset 'Khusus VPN' (2.730 flow) setelah kurasi dinamis. Tugas Biner secara alami tidak lagi disertakan dalam konteks ini).}
    \label{fig:vpn_only_dist}
\end{figure}

\section{Perancangan dan Implementasi Hybrid Flow Vector (HFV)}

\subsection{Pengantar Kerangka Kerja HFV: $\Omega = (\alpha, \beta, \gamma)$}
\label{subsec:kerangka_hfv}

Klasifikasi lalu lintas jaringan terenkripsi merupakan tantangan fundamental dalam manajemen jaringan modern dan keamanan siber. Teknik tradisional seperti \textit{Deep Packet Inspection} (DPI) menjadi tidak efektif karena enkripsi menyembunyikan konten \textit{payload} \citep{razooqi2025vpn}. Akibatnya, penelitian beralih ke analisis metadata dan karakteristik perilaku \textit{flow} untuk melakukan identifikasi.

Pendekatan yang ada saat ini umumnya terbagi menjadi dua kategori: (1) model berbasis statistik, yang menganalisis fitur-fitur level \textit{flow} seperti ukuran paket dan \textit{Inter-Arrival Time} (IAT) \citep{DraperGil2016}, dan (2) model berbasis \textit{Deep Learning} (DL), yang dapat belajar representasi kompleks langsung dari data mentah, seperti \textit{raw packet payload} \citep{Lotfollahi2019}.

Namun, kedua pendekatan memiliki keterbatasan jika digunakan secara terpisah. Model statistik murni mungkin gagal menangkap pola-pola rumit di level \textit{byte} yang tersembunyi di dalam \textit{payload} terenkripsi. Sebaliknya, model DL yang hanya berfokus pada \textit{payload} mungkin kehilangan konteks perilaku makroskopis dari keseluruhan \textit{flow}.

Untuk mengatasi keterbatasan ini, penelitian ini merancang dan mengimplementasikan sebuah kerangka kerja representasi fitur hibrida, yang disebut \textbf{\textit{Hybrid Flow Vector} (HFV)}. Hipotesis utamanya adalah bahwa dengan menggabungkan dua "pandangan" yang berbeda dari sebuah \textit{flow}---yaitu pola mikroskopis \textit{level-byte} dan perilaku makroskopis \textit{level-flow}---kita dapat menciptakan sebuah "sidik jari" (\textit{fingerprint}) yang jauh lebih kaya dan tangguh untuk klasifikasi. Pendekatan hibrida yang menggabungkan berbagai jenis fitur telah menunjukkan potensi besar dalam penelitian terkait \citep{jorgensen2023extensible}.

Secara formal, HFV ($\Omega$) didefinisikan sebagai gabungan dari tiga komponen vektor yang berbeda:
\begin{equation}
    \Omega = \alpha \oplus \beta \oplus \gamma
\end{equation}
dimana $\oplus$ melambangkan operasi konkatenasi (penggabungan) vektor.

Setiap komponen dirancang untuk menangkap aspek unik dari \textit{flow} jaringan:
\begin{itemize}
    \item \textbf{Komponen $\alpha$ (alpha)}: Sebuah vektor fitur \textit{deep learning} yang diekstraksi dari \textit{raw payload} paket-paket pertama \textit{flow}. Komponen ini mewakili \textbf{pola level-byte} dan dirancang untuk menangkap tanda tangan (\textit{signature}) mikroskopis yang mungkin spesifik untuk aplikasi tertentu, yang diekstraksi menggunakan arsitektur \textit{1D-Convolutional Neural Network} (1D-CNN).

    \item \textbf{Komponen $\beta$ (beta)}: Sebuah vektor fitur statistik yang komprehensif dari \textbf{keseluruhan \textit{flow}} (\textit{flow-level}). Komponen ini mencakup statistik deskriptif (mean, std, min, max, dll.) dari ukuran paket dan IAT untuk seluruh durasi \textit{flow}, yang menggambarkan perilaku makroskopis.

    \item \textbf{Komponen $\gamma$ (gamma)}: Sebuah vektor fitur statistik yang berfokus pada \textbf{level \textit{burst}} komunikasi. Komponen ini dirancang untuk menangkap pola komunikasi \textit{on-off} (kirim-tunggu-kirim) yang sering terjadi pada aplikasi interaktif atau \textit{streaming}.
\end{itemize}

Dengan mengintegrasikan ketiga komponen ini, vektor HFV $\Omega$ menyediakan representasi multi-modal yang holistik dari setiap \textit{flow}, yang menjadi dasar untuk proses klasifikasi \textit{machine learning} yang akan dijelaskan selanjutnya. Bagian-bagian berikut akan merinci perancangan dan implementasi teknis dari ekstraksi masing-masing komponen $\alpha$, $\beta$, dan $\gamma$.

\subsection{Komponen \texorpdfstring{$\alpha$}{alpha}: Ekstraksi Fitur 1D-CNN dari Pola Byte-Level}
\label{subsec:komponen_alpha}

Komponen $\alpha$ dirancang untuk menangkap \textbf{pola mikroskopis level-byte} yang ada di dalam \textit{payload} terenkripsi. Berbeda dengan analisis statistik ($\beta$ dan $\gamma$) yang melihat \textit{flow} dari luar, komponen $\alpha$ mencoba "mengintip" ke dalam data mentah itu sendiri.

Literatur telah menunjukkan bahwa meskipun terenkripsi, \textit{raw packet payload} seringkali masih mengandung pola-pola yang dapat dipelajari oleh model \textit{Deep Learning}, khususnya \textit{1D-Convolutional Neural Networks} (1D-CNN) \citep{wang2017end, Lotfollahi2019}. Pola-pola ini dapat berupa artefak dari protokol aplikasi, \textit{padding}, atau tanda tangan (\textit{signature}) unik lainnya yang tidak dihilangkan oleh enkripsi.

Proses untuk menghasilkan vektor fitur $\alpha$ dibagi menjadi dua fase utama, yang diimplementasikan dalam dua skrip Python terpisah: (1) Ekstraksi Data Payload Mentah, dan (2) Pelatihan Encoder dan Generasi Fitur.

\subsubsection{Fase 1: Ekstraksi Data \textit{Raw Payload}}
\label{ssubsec:alpha_fase1}

Fase pertama, yang diimplementasikan dalam skrip \texttt{alpha\_1.py}, bertanggung jawab untuk mengubah \textit{flow} \texttt{.pcap} bersih (hasil pra-pemrosesan) menjadi format dataset yang siap digunakan oleh CNN. Logika inti dari skrip ini dijelaskan dalam Kode \ref{lst:pseudo_alpha1}.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\begin{algorithmic}[1]
    \State \textbf{Definisi:} N\_PACKETS $\gets 10$, PAYLOAD\_LEN $\gets 784$
    \State \textbf{Input:} SOURCE\_DIR (direktori .pcap)
    \State \textbf{Output:} OUTPUT\_DATA\_FILE (.npy), OUTPUT\_LABELS\_FILE (.csv)
    \item[]
    \Function{ProcessPcap}{file}
        \State $matriks\_flow \gets \Call{ArrayNol}{N\_PACKETS, PAYLOAD\_LEN}$
        \State $label \gets \Call{DapatkanLabel}{file}$
        \State $packets \gets \Call{BacaPcap}{file}$
        \State $idx \gets 0$
        \For{setiap $pkt$ \textbf{in} $packets$ dan $idx < N\_PACKETS$}
            \State $payload \gets \Call{DapatkanPayload}{pkt}$
            \If{$payload \neq null$}
                \State $vektor \gets \Call{ArrayNol}{PAYLOAD\_LEN}$
                \State $n \gets \min(\Call{panjang}{payload}, PAYLOAD\_LEN)$
                \State $vektor[0...n] \gets \Call{KonversiKeByte}{payload, n} / 255.0$
                \State $matriks\_flow[idx] \gets vektor$
                \State $idx \gets idx + 1$
            \EndIf
        \EndFor
        \State \Return $matriks\_flow, label$
    \EndFunction
    \item[]
    \Procedure{Main}{}
        \State $files \gets \Call{DapatkanSemuaFilePcap}{SOURCE\_DIR}$
        \State $(dataX, labelY) \gets \Call{JalankanParalel}{ProcessPcap, files}$
        \State $arrayX \gets \Call{TumpukArray}{dataX}$
        \State \Call{SimpanNpy}{arrayX, outputDataFile}
        \State \Call{SimpanCsv}{labelY, outputLabelsFile}
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode untuk Fase 1: Ekstraksi Data Payload Mentah.}
\label{lst:pseudo_alpha1}
\end{listing}

Implementasi spesifik dari langkah ekstraksi dan normalisasi \textit{payload} (langkah 3, 4, dan 5 dalam pseudocode) ditunjukkan pada Kode \ref{lst:snippet_alpha1}. Bagian ini adalah inti dari skrip \texttt{alpha\_1.py}, di mana setiap byte dinormalisasi menjadi nilai antara 0.0 dan 1.0.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari alpha_1.py
# (dalam fungsi process_pcap_for_cnn)

def process_pcap_for_cnn(pcap_filepath):
    try:
        # ... (kode pengambilan label) ...

        # This will be our (10, 784) array for this one flow
        flow_data = np.zeros((N_PACKETS, PAYLOAD_LEN), dtype=np.float32)

        packets = rdpcap(pcap_filepath)
        packet_count = 0

        for pkt in packets:
            if packet_count >= N_PACKETS:
                break

            # Find the payload (bytes *after* TCP/UDP header)
            payload = None
            if TCP in pkt:
                payload = bytes(pkt[TCP].payload)
            elif UDP in pkt:
                payload = bytes(pkt[UDP].payload)
            
            if not payload: # Skip jika tidak ada payload
                continue

            # This is our (784,) vector for this one packet
            normalized_payload = np.zeros(PAYLOAD_LEN, dtype=np.float32)
            copy_len = min(len(payload), PAYLOAD_LEN)

            # Copy data dari byte buffer dan normalisasi (0-255 -> 0.0-1.0)
            byte_data = np.frombuffer(payload[:copy_len], dtype=np.uint8)
            normalized_payload[:copy_len] = byte_data.astype(np.float32) / 255.0

            # Add the packet vector to our flow matrix
            flow_data[packet_count] = normalized_payload
            packet_count += 1
        
        # ... (kode return) ...
    except Exception as e:
        return None
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode yang menunjukkan proses normalisasi payload.}
    \label{lst:snippet_alpha1}
\end{listing}

Hasil akhir dari fase ini adalah dua file:
\begin{itemize}
    \item \texttt{VPNOnly-cnn\_payload\_data.npy}: Sebuah \textit{array} NumPy 3-dimensi dengan bentuk \texttt{(jumlah\_sampel, 10, 784)}.
    \item \texttt{VPNOnly-cnn\_payload\_labels.csv}: File CSV yang berisi label (filename, aplikasi, kategori) yang sesuai untuk setiap sampel.
\end{itemize}

\subsubsection{Fase 2: Pelatihan Encoder \& Generasi Fitur}
\label{ssubsec:alpha_fase2}

Fase kedua, diimplementasikan dalam \texttt{alpha\_2.py}, menggunakan data \texttt{.npy} yang dihasilkan Fase 1 untuk melatih model \textit{Deep Learning} dan mengekstrak fitur $\alpha$. Logika skrip ini diringkas dalam Kode \ref{lst:pseudo_alpha2}.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\begin{algorithmic}[1]
    \State \textbf{Definisi:} FEATURE\_VECTOR\_SIZE $\gets 128$
    \State \textbf{Input:} DATA\_FILE (.npy), LABELS\_FILE (.csv)
    \State \textbf{Output:} OUTPUT\_ALPHA\_V3\_FILE (.csv), OUTPUT\_ENCODER\_MODEL\_FILE (.keras)
    \item[]
    \Function{BuildModel}{numClasses}
        \State $input \gets \Call{Input}{(7840, 1)}$
        \State $x \gets \Call{Conv1D}{32} \to \Call{MaxPooling1D}{4}$
        \State $x \gets \Call{Conv1D}{64} \to \Call{MaxPooling1D}{4}$
        \State $x \gets \Call{Conv1D}{128} \to \Call{MaxPooling1D}{4}$
        \State $x \gets \Call{Flatten}{x}$
        \State $x \gets \Call{Dense}{FEATURE\_VECTOR\_SIZE, \text{name}=\text{"encoder\_output"}}$
        \State $x \gets \Call{Dropout}{0.5}(x)$
        \State $output \gets \Call{Dense}{numClasses, \text{activation}=\text{"softmax"}}$
        \State $model \gets \Call{Model}{input, output}$
        \State \Call{Compile}{model, \text{optimizer}=\text{"adam"}}
        \State \Return $model$
    \EndFunction
    \item[]
    \Procedure{Main}{}
        \State $(X, y, labels) \gets \Call{LoadData}{}$
        \State $X \gets \Call{Reshape}{X, (N, 7840, 1)}$
        \State $y \gets \Call{ToCategorical}{y}$
        \State $model \gets \Call{BuildModel}{numClasses}$
        \State $(X_{train}, X_{val}, y_{train}, y_{val}) \gets \Call{Split}{X, y}$
        \State \Call{Fit}{model, $X_{train}$, $y_{train}$, validation: $(X_{val}, y_{val})$}
        \State $encoder \gets \Call{Model}{model.input, \text{layer["encoder\_output"]}}$
        \State $features \gets \Call{Predict}{encoder, X}$ \Comment{Bentuk: (N, 128)}
        \State $df \gets \Call{Gabung}{labels, features}$
        \State \Call{SimpanCsv}{df, outputAlphaFile}
        \State \Call{Save}{encoder, outputEncoderFile}
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode untuk Fase 2: Pelatihan Encoder dan Generasi Fitur $\alpha$.}
\label{lst:pseudo_alpha2}
\end{listing}

Seperti dijelaskan dalam pseudocode, sebuah model 1D-CNN dibangun (Kode \ref{lst:snippet_alpha2}). Tujuan utamanya adalah untuk melatih lapisan \textit{Dense} (neuron=128) agar menjadi ekstraktor fitur yang baik. Lapisan ini diberi nama "encoder\_output" agar dapat diekstraksi dengan mudah setelah pelatihan.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari alpha_2.py
# (Fungsi build_model)

def build_model(num_classes):
    print("Building 1D-CNN model...")

    input_layer = Input(shape=INPUT_SHAPE) # INPUT_SHAPE = (7840, 1)

    # Convolutional Block 1
    x = Conv1D(filters=32, kernel_size=7, activation='relu', padding='same')(input_layer)
    x = MaxPooling1D(pool_size=4)(x)

    # Convolutional Block 2
    x = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(x)
    x = MaxPooling1D(pool_size=4)(x)

    # Convolutional Block 3
    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
    x = MaxPooling1D(pool_size=4)(x)

    x = Flatten()(x)

    # --- This is our Feature Vector ---
    # We give it a name so we can easily extract it later
    x = Dense(FEATURE_VECTOR_SIZE, activation='relu', name="encoder_output")(x)
    x = Dropout(0.5)(x)
    # ----------------------------------

    # Output classifier layer (Hanya untuk pelatihan)
    output_layer = Dense(num_classes, activation='softmax', name="classifier_output")(x)

    # Create the full model
    model = Model(inputs=input_layer, outputs=output_layer)
    
    # ... (kode compile) ...
    return model
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{alpha\_2.py} menunjukkan arsitektur 1D-CNN.}
    \label{lst:snippet_alpha2}
\end{listing}

Model \textit{lengkap} dilatih pada 80\% data untuk mengklasifikasikan \textbf{aplikasi}. Dengan "memaksa" model membedakan 'Skype' dari 'Netflix', lapisan "encoder\_output" (Dense 128) belajar menghasilkan representasi vektor 128-dimensi yang informatif.

Setelah pelatihan selesai, bagian \textit{classifier} dibuang, dan model \textit{encoder} yang sudah "pintar" ini digunakan untuk memprediksi \textit{seluruh} dataset. Proses ekstraksi ini ditunjukkan pada Kode \ref{lst:snippet_alpha3}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari alpha_2.py
# (dari fungsi main)

    # ... (Kode memuat data dan melatih 'model') ...

    print("Extracting and saving the encoder model...")

    # 4. Buat model encoder baru dengan "memotong" model yang sudah dilatih
    encoder_model = Model(
        inputs=model.input,
        outputs=model.get_layer("encoder_output").output
    )

    encoder_model.save(OUTPUT_ENCODER_MODEL_FILE)

    # 5. Gunakan encoder untuk mengekstrak fitur dari *seluruh* dataset
    print(f"Generating {FEATURE_VECTOR_SIZE}-dim features for all {X_full.shape[0]} samples...")
    alpha_prime_prime_features = encoder_model.predict(X_full, batch_size=128)

    # ... (Kode menyimpan 'alpha_prime_prime_features' ke CSV) ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode yang menunjukkan proses ekstraksi fitur $\alpha$.}
    \label{lst:snippet_alpha3}
\end{listing}

Hasil dari Fase 2 adalah \textit{array} 2D \texttt{(jumlah\_sampel, 128)}. Ini adalah \textbf{komponen fitur $\alpha$} final, yang kemudian disimpan ke file CSV untuk digabungkan dengan komponen $\beta$ dan $\gamma$.

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        node distance=2.5cm and 2cm, % Jarak vertikal & horizontal lebih longgar
        block/.style = {
            rectangle, draw, thick, fill=blue!10,
            text width=11em, text centered, rounded corners,
            minimum height=3em, drop shadow
        },
        data_node/.style = {
            cylinder, draw, thick, fill=orange!10,
            shape aspect=0.3, minimum height=2.5em, text centered,
            text width=10em, drop shadow
        },
        label_node/.style = {
            cylinder, draw, thick, fill=green!10, shape aspect=0.3,
            minimum height=2.5em, text centered, text width=10em, 
            drop shadow
        },
        arrow/.style = {
            thick, ->, >=Stealth
        }
    ]
        
        \small
        
        % --- Alur Vertikal ---
        
        % Fase 1: Ekstraksi Payload
        \node (pcap) [data_node] {Flow .pcap Bersih};
        
        \node (step1) [block, below=of pcap] {Fase 1: Ekstraksi Payload \texttt{(...1.py)}};
        
        % Node untuk keterangan Fase 1
        \node (desc1) [below=0.4cm of step1, text width=11em, align=center, font=\tiny] 
            {10 pkt $\times$ 784 byte.\\Normalisasi 0.0-1.0.};
        
        \node (npy) [data_node, below=3cm of step1] {Data Payload Mentah};
        
        % Node untuk keterangan bentuk data
        \node (desc_npy) [below=0.4cm of npy, font=\tiny] 
            {Bentuk: (N, 7840, 1)};
        
        % Panah Fase 1
        \draw [arrow] (pcap) -- (step1);
        \draw [arrow] (step1) -- (npy);
        
        % Fase 2: Input Pelatihan
        \node (labels) [label_node, right=2.5cm of npy, yshift=1cm] {Label Aplikasi};
        \node (desc_labels) [below=0.4cm of labels, font=\tiny, xshift=2mm] 
            {(Mis: 'Skype')};
        
        % Model container
        \node (model) [block, fill=red!5, below=4cm of npy, text width=13em, 
                      minimum height=10em] {};
        
        % Node internal dalam model dengan positioning relatif yang lebih rapi
        \node (conv) [draw, fill=white, text width=11em, text centered, 
                     minimum height=2em, drop shadow, anchor=north] 
                     at ([yshift=-0.6cm]model.north) {Blok Konvolusi (Conv/Pool)};
        \node (encoder) [draw, fill=yellow!20, text width=11em, text centered, 
                        minimum height=2em, drop shadow, below=1cm of conv] 
                        {Lapisan Encoder (Dense 128)};
        \node (classifier) [draw, fill=white, text width=11em, text centered, 
                           minimum height=2em, drop shadow, below=1cm of encoder] 
                           {Lapisan Klasifikasi (Softmax)};
        
        % Judul model
        \node [above=0.2cm of model.north, font=\small, anchor=south] 
              {Fase 2: Arsitektur 1D-CNN \texttt{(...2.py)}};
        
        % Panah Pelatihan dengan posisi label yang lebih jelas
        \draw [arrow] (npy) -- node[left, pos=0.3, font=\tiny] {(Data Latih)} (model.north);
        \draw [arrow] (labels.west) -| node[above right, pos=0.2, font=\tiny] {(Target Latih)} (model.east);
        
        % Fase 2b: Generasi Fitur
        \node (alpha) [data_node, fill=yellow!20, below=4.5cm of model] {Komponen Fitur $\alpha$};
        \node (desc_alpha) [below=0.4cm of alpha, font=\tiny] 
            {Bentuk: (N, 128)};
        
        % Panah Ekstraksi dengan label yang dipendekkan dan posisi yang lebih baik
        \draw [arrow, line width=1.5pt] (model.south) -- node[right, pos=0.6, align=center, 
            text width=7em, font=\tiny] {Ekstraksi fitur dari\\lapisan \textbf{encoder}} (alpha.north);
        
    \end{tikzpicture}
    \caption{Diagram alir proses ekstraksi komponen fitur $\alpha$ (alpha). Data payload mentah diekstrak dari \texttt{.pcap} (Fase 1), kemudian sebuah 1D-CNN dilatih untuk mengklasifikasikan aplikasi. Lapisan \textit{encoder} (Dense 128) dari model yang telah dilatih kemudian digunakan untuk menghasilkan vektor fitur $\alpha$ akhir.}
    \label{fig:alpha_extraction_vertical}
\end{figure}

\subsection{Komponen \texorpdfstring{$\beta$}{beta}: Statistik Level Flow}
\label{subsec:komponen_beta}

Komponen $\beta$ dirancang untuk menangkap \textbf{pola makroskopis} dari keseluruhan \textit{flow} jaringan. Komponen ini didasarkan pada hipotesis bahwa bahkan ketika \textit{payload} dienkripsi, karakteristik perilaku statistik dari aliran---seperti seberapa sering, seberapa besar, dan kapan paket dikirim---dapat mengungkapkan informasi penting tentang aplikasi yang mendasarinya \citep{DraperGil2016}.

Komponen ini (disebut sebagai \textit{Delta ($\delta$)} dalam dokumentasi proyek awal) menggantikan komponen $\beta$ asli dari model PESV, yang bergantung pada perbandingan prototipe yang ambigu (seperti dijelaskan dalam \ref{cha:pendahuluan}). Komponen $\beta$ yang baru ini jauh lebih tangguh karena secara langsung menghitung profil statistik deskriptif dari aliran itu sendiri, tanpa memerlukan perbandingan eksternal.

Implementasi ekstraksi fitur $\beta$ dilakukan oleh skrip \texttt{v2\_beta.py} (merujuk pada \texttt{v2-delta.ipynb}). Logika inti dari skrip ini diringkas dalam Pseudocode \ref{lst:pseudo_beta}.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\begin{algorithmic}[1]
    \State \textbf{Input:} FLOW\_DIR (direktori .pcap)
    \State \textbf{Output:} OUTPUT\_CSV (file .csv berisi fitur beta)
    \item[]
    \Function{ProcessPcap}{file}
        \State $(app, cat, type) \gets \Call{GetFlowLabels}{file}$
        \If{$app = null$} \Return $null$ \EndIf
        \State $packets \gets \Call{BacaPcap}{file}$
        \State $clientIP \gets \Call{GetSourceIP}{packets[0]}$
        \State $(sizesC2S, timesC2S) \gets \Call{GetPackets}{packets, clientIP, arahC2S}$
        \State $(sizesS2C, timesS2C) \gets \Call{GetPackets}{packets, clientIP, arahS2C}$
        \State $iatsC2S \gets \Call{HitungSelisih}{timesC2S}$
        \State $iatsS2C \gets \Call{HitungSelisih}{timesS2C}$
        \State $fitur \gets \{\}$
        \State $stats1 \gets \Call{CalcStats}{sizesC2S}$
        \State $stats2 \gets \Call{CalcStats}{sizesS2C}$
        \State $stats3 \gets \Call{CalcStats}{iatsC2S}$
        \State $stats4 \gets \Call{CalcStats}{iatsS2C}$
        \State $stats5 \gets \Call{HitungFiturFlow}{timesC2S, timesS2C}$
        \State $fitur \gets \Call{Gabung}{stats1, stats2, stats3, stats4, stats5}$
        \State \Return $(app, cat, type, fitur)$
    \EndFunction
    \item[]
    \Procedure{Main}{}
        \State $files \gets \Call{GetAllPcap}{FLOW\_DIR}$
        \State $hasil \gets \Call{JalankanParalel}{ProcessPcap, files}$
        \State $df \gets \Call{ToDataFrame}{hasil}$
        \State \Call{SimpanCsv}{df, outputFile}
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode untuk Ekstraksi Komponen $\beta$ (Statistik Level Flow).}
\label{lst:pseudo_beta}
\end{listing}

Kunci dari komponen $\beta$ adalah analisis \textbf{bidireksional}. Untuk setiap \textit{flow}, skrip terlebih dahulu mengidentifikasi IP klien (diasumsikan sebagai IP sumber dari paket pertama), seperti terlihat pada Kode \ref{lst:snippet_beta1}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari v2_beta.py
# (dalam fungsi process_pcap_file)

    # ... (kode inisialisasi list) ...
    try:
        packets = rdpcap(filepath)

        client_ip = None
        for pkt in packets:
            if IP in pkt:
                client_ip = pkt[IP].src
                break

        if client_ip is None:
            return None # Skip non-IP flows
        # ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{v2\_beta.py} menunjukkan identifikasi IP klien.}
    \label{lst:snippet_beta1}
\end{listing}

Setelah IP klien diketahui, skrip melakukan iterasi kedua melalui semua paket untuk memisahkan ukuran dan waktu kedatangan paket ke dalam empat daftar terpisah:
\begin{itemize}
    \item \texttt{c2s\_sizes}: Ukuran paket dari Klien ke Server.
    \item \texttt{c2s\_times}: Waktu kedatangan paket dari Klien ke Server.
    \item \texttt{s2c\_sizes}: Ukuran paket dari Server ke Klien.
    \item \texttt{s2c\_times}: Waktu kedatangan paket dari Server ke Klien.
\end{itemize}

Selanjutnya, waktu kedatangan (misalnya \texttt{c2s\_times}) digunakan untuk menghitung \textit{Inter-Arrival Times} (IAT) untuk setiap arah. Kode \ref{lst:snippet_beta2} menunjukkan bagaimana daftar IAT ini dihitung menggunakan fungsi \texttt{np.diff}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari v2_beta.py
# (dalam fungsi process_pcap_file)

    # ... (kode ekstraksi paket bidireksional) ...

    # 5. Calculate Inter-Arrival Times (IATs)
    # np.diff computes the difference between consecutive elements
    c2s_iats = np.diff(c2s_times).tolist()
    s2c_iats = np.diff(s2c_times).tolist()

    # 6. Calculate all statistical features
    features = {}
    # ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{v2\_beta.py} menunjukkan kalkulasi IAT bidireksional.}
    \label{lst:snippet_beta2}
\end{listing}

Terakhir, fungsi utilitas \texttt{calculate\_stats} (ditampilkan dalam Kode \ref{lst:snippet_beta3}) dipanggil pada keempat daftar ini (\texttt{c2s\_sizes}, \texttt{s2c\_sizes}, \texttt{c2s\_iats}, \texttt{s2c\_iats}).

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari v2_beta.py
# (Fungsi calculate_stats)

def calculate_stats(data_list, prefix):
    """
    Calculates a full statistical profile for a list of numbers.
    Returns a dictionary of features.
    """
    stats = {}

    # Ensure all keys exist, even if they are 0
    stat_names = ['count', 'sum', 'mean', 'std', 'min', 'max', 'median', 'p25', 'p75']
    for name in stat_names:
        stats[f"{prefix}_{name}"] = 0.0

    if not data_list:
        return stats # Return all zeros

    arr = np.array(data_list)

    stats[f"{prefix}_count"] = float(arr.size)
    stats[f"{prefix}_sum"] = float(np.sum(arr))
    stats[f"{prefix}_mean"] = float(np.mean(arr))
    stats[f"{prefix}_min"] = float(np.min(arr))
    stats[f"{prefix}_max"] = float(np.max(arr))
    stats[f"{prefix}_median"] = float(np.median(arr))

    # Percentiles
    stats[f"{prefix}_p25"] = float(np.percentile(arr, 25))
    stats[f"{prefix}_p75"] = float(np.percentile(arr, 75))

    # Std deviation requires at least 2 samples
    if arr.size > 1:
        stats[f"{prefix}_std"] = float(np.std(arr))

    return stats
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{v2\_beta.py} menunjukkan fungsi utilitas kalkulasi 9 statistik.}
    \label{lst:snippet_beta3}
\end{listing}

Fungsi ini menghitung 9 statistik deskriptif: \textit{count} (jumlah), \textit{sum} (total), \textit{mean} (rata-rata), \textit{std} (standar deviasi), \textit{min}, \textit{max}, \textit{median}, \textit{p25} (kuartil 1), dan \textit{p75} (kuartil 3).

Karena 9 statistik ini dihitung untuk 4 daftar yang berbeda (ukuran c2s, ukuran s2c, IAT c2s, IAT s2c), total $9 \times 4 = 36$ fitur statistik bidireksional dihasilkan. Ditambah dengan 3 fitur level \textit{flow} total (durasi, total paket, total volume), komponen $\beta$ akhir terdiri dari \textbf{39 fitur} statistik.

\subsection{Komponen \texorpdfstring{$\gamma$}{gamma}: Statistik Level Burst}
\label{subsec:komponen_gamma}

Komponen $\gamma$ (gamma) dirancang untuk menangkap \textbf{pola komunikasi \textit{on-off}} dari sebuah aliran. Analisis ini berbeda dari komponen $\beta$, yang melihat aliran sebagai satu kesatuan statistik, dan komponen $\alpha$, yang melihat \textit{payload} mentah. Komponen $\gamma$ berfokus pada ritme komunikasi.

Banyak aplikasi, terutama yang bersifat interaktif (seperti \textit{chat} atau VoIP) atau \textit{streaming} (seperti video), tidak mengirimkan data dalam aliran yang konstan. Sebaliknya, mereka mengirimkan sekelompok paket (\textit{burst}) data, diikuti dengan jeda (waktu diam), lalu diikuti dengan \textit{burst} data berikutnya. Menganalisis karakteristik dari \textit{burst} dan jeda ini dapat memberikan sidik jari yang kuat untuk mengidentifikasi jenis aplikasi..

\subsubsection{Definisi Burst}
Kunci dari analisis ini adalah definisi "burst". Dalam penelitian ini, sebuah \textit{burst} didefinisikan sebagai sekelompok paket (satu atau lebih) di mana waktu jeda antar paket tersebut \textbf{kurang dari 1.0 detik}. Jika jeda antar dua paket melebihi ambang batas ini, \textit{burst} sebelumnya dianggap selesai dan \textit{burst} baru dimulai. Nilai ambang batas 1.0 detik ini diatur dalam variabel \texttt{BURST\_IDLE\_THRESHOLD}.

\subsubsection{Implementasi Ekstraksi (gamma.py)}
Ekstraksi fitur $\gamma$ diimplementasikan dalam skrip \texttt{gamma.py}. Berbeda dengan komponen $\beta$, analisis \textit{burst} ini bersifat \textbf{unidireksional} dalam arti menggabungkan paket dari kedua arah (Klien $\leftrightarrow$ Server) menjadi satu urutan waktu sebelum diproses.

Logika inti dari skrip ini diringkas dalam Kode \ref{lst:pseudo_gamma}.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\begin{algorithmic}[1]
    \State \textbf{Definisi:} THRESHOLD $\gets 1.0$ (detik)
    \State \textbf{Input:} file (.pcap)
    \State \textbf{Output:} fitur\_gamma (37 fitur statistik burst)
    \item[]
    \Function{DeteksiBurst}{file}
        \State $packets \gets \Call{GetAllPacketsSorted}{file}$
        \State $(counts, volumes, durations, idles) \gets ([], [], [], [])$
        \State $(firstTime, firstSize) \gets packets[0]$
        \State $(burstCount, burstVolume, burstStart, lastTime) \gets (1, firstSize, firstTime, firstTime)$
        \For{each $(t, s)$ in $packets[1...]$}
            \State $idle \gets t - lastTime$
            \If{$idle < THRESHOLD$}
                \State $burstCount \gets burstCount + 1$
                \State $burstVolume \gets burstVolume + s$
            \Else
                \State \Call{Append}{durations, $lastTime - burstStart$}
                \State \Call{Append}{counts, burstCount}
                \State \Call{Append}{volumes, burstVolume}
                \State \Call{Append}{idles, idle}
                \State $(burstCount, burstVolume, burstStart) \gets (1, s, t)$
            \EndIf
            \State $lastTime \gets t$
        \EndFor
        \State \Call{Append}{durations, $lastTime - burstStart$}
        \State \Call{Append}{counts, burstCount}
        \State \Call{Append}{volumes, burstVolume}
        \State $fitur \gets \{\}$
        \State $fitur[\text{totalBurst}] \gets \Call{Length}{counts}$
        \State $s1 \gets \Call{CalcStats}{counts}$
        \State $s2 \gets \Call{CalcStats}{volumes}$
        \State $s3 \gets \Call{CalcStats}{durations}$
        \State $s4 \gets \Call{CalcStats}{idles}$
        \State $fitur \gets \Call{Gabung}{fitur, s1, s2, s3, s4}$
        \State \Return $fitur$
    \EndFunction
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode untuk Ekstraksi Komponen $\gamma$ (Statistik Level Burst).}
\label{lst:pseudo_gamma}
\end{listing}

Kode \ref{lst:snippet_gamma1} menunjukkan logika inti dari algoritma deteksi \textit{burst} ini. Skrip mengiterasi setiap paket (mulai dari paket kedua) dan menghitung \texttt{idle\_time} (waktu jeda) dari paket sebelumnya.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari gamma.py
# (dalam fungsi process_pcap_file)

    # ... (kode inisialisasi burst pertama) ...
    last_packet_time = all_packets[0][0]

    # Iterate from the *second* packet onwards
    for (pkt_time, pkt_size) in all_packets[1:]:
        idle_time = pkt_time - last_packet_time

        if idle_time < BURST_IDLE_THRESHOLD:
            # --- This packet is PART of the current burst ---
            current_burst_packets += 1
            current_burst_volume += pkt_size
        else:
            # --- This packet is the START of a new burst ---
            # 1. Save the previous burst
            burst_duration = last_packet_time - current_burst_start_time
            burst_packet_counts.append(current_burst_packets)
            burst_volumes.append(current_burst_volume)
            burst_durations.append(burst_duration)

            # 2. Save the idle time that just ended
            burst_idle_times.append(idle_time)

            # 3. Reset for the new burst
            current_burst_packets = 1
            current_burst_volume = pkt_size
            current_burst_start_time = pkt_time

        # Update the time of the last packet seen
        last_packet_time = pkt_time

    # 6. Save the *final* burst after the loop
    burst_duration = last_packet_time - current_burst_start_time
    # ... (append burst terakhir ke list) ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{gamma.py} menunjukkan logika deteksi batas burst.}
    \label{lst:snippet_gamma1}
\end{listing}

Jika \texttt{idle\_time} kurang dari ambang batas (1.0 detik), paket saat ini ditambahkan ke \textit{burst} yang sedang berjalan. Jika tidak, \textit{burst} sebelumnya disimpan, dan \texttt{idle\_time} dicatat sebagai waktu jeda antar-\textit{burst}.

Setelah semua paket diproses, skrip memanggil fungsi \texttt{calculate\_stats} (yang sama dengan yang digunakan pada komponen $\beta$) pada empat daftar yang dihasilkan:
\begin{itemize}
    \item \texttt{burst\_packet\_counts}: Daftar jumlah paket untuk setiap burst.
    \item \texttt{burst\_volumes}: Daftar total volume (byte) untuk setiap burst.
    \item \texttt{burst\_durations}: Daftar durasi (detik) untuk setiap burst.
    \item \texttt{burst\_idle\_times}: Daftar waktu jeda (detik) antar burst.
\end{itemize}

Menghitung 9 statistik (count, mean, std, dll.) untuk 4 daftar ini menghasilkan $9 \times 4 = 36$ fitur. Ditambah dengan satu fitur tambahan, \texttt{total\_burst\_count}, komponen $\gamma$ akhir terdiri dari \textbf{37 fitur} statistik level \textit{burst}.

\subsection{Integrasi Komponen HFV dan Pembuatan Basis Data Fitur}
\label{subsec:integrasi_hfv}

Setelah ketiga komponen fitur diekstraksi secara terpisah dalam tiga \textit{pipeline} yang independen, langkah terakhir adalah mengintegrasikannya menjadi satu vektor fitur hibrida (HFV) tunggal. Proses ini diimplementasikan dalam skrip \texttt{merge\_hfv.py}.

Tujuan dari skrip ini adalah untuk membuat satu basis data (file \texttt{.csv}) di mana setiap baris mewakili satu \textit{flow} yang unik, dan kolom-kolomnya berisi:
\begin{enumerate}
    \item Informasi label (misalnya, \texttt{filename}, \texttt{application}, \texttt{category}).
    \item 128 fitur dari komponen $\alpha$ (pola \textit{byte} 1D-CNN).
    \item 39 fitur dari komponen $\beta$ (statistik level \textit{flow}).
    \item 37 fitur dari komponen $\gamma$ (statistik level \textit{burst}).
\end{enumerate}

Penggabungan ini menghasilkan vektor HFV $\Omega$ final dengan total $128 + 39 + 37 = \mathbf{204}$ fitur untuk setiap \textit{flow}.

Proses penggabungan ini diringkas dalam Kode \ref{lst:merge_hfv}. Kunci dari proses ini adalah penggunaan \texttt{filename} sebagai \textit{primary key} unik untuk menggabungkan semua \textit{DataFrame} komponen.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\caption{Pseudocode Integrasi Komponen HFV}
\begin{algorithmic}[1]
    \State \textbf{Input:}
    \State \quad $File_{label} \gets$ \texttt{VPNOnly-cnn\_payload\_labels.csv}
    \State \quad $File_{\alpha} \gets$ \texttt{VPNOnly-alpha\_double\_prime\_component\_v3.csv}
    \State \quad $File_{\beta} \gets$ \texttt{VPNOnly-delta\_component\_v2.csv}
    \State \quad $File_{\gamma} \gets$ \texttt{VPNOnly-gamma\_prime\_component\_v2.csv}
    \State \textbf{Output:} $File_{HFV} \gets$ \texttt{VPNOnly-final\_PESV\_dataset\_v3.csv}
    
    \State \Comment{Muat semua komponen ke dalam DataFrame pandas}
    \State $df_{label} \gets \textbf{load\_csv}(File_{label})$ \Comment{Basis label (9,720 baris)}
    \State $df_{\alpha} \gets \textbf{load\_csv}(File_{\alpha})$ \Comment{Fitur Alpha (9,720 baris)}
    \State $df_{\beta} \gets \textbf{load\_csv}(File_{\beta})$ \Comment{Fitur Beta (~10k baris)}
    \State $df_{\gamma} \gets \textbf{load\_csv}(File_{\gamma})$ \Comment{Fitur Gamma (~10k baris)}
    
    \State \Comment{Persiapan merge: Hapus kolom label duplikat dari Beta dan Gamma}
    \State $cols\_to\_drop \gets$ ['application', 'category', 'binary\_type']
    \State $df_{\beta\_feat} \gets \textbf{drop\_columns}(df_{\beta}, cols\_to\_drop)$
    \State $df_{\gamma\_feat} \gets \textbf{drop\_columns}(df_{\gamma}, cols\_to\_drop)$
    
    \State \Comment{Lakukan penggabungan inner-join sekuensial}
    \State $df_{merged} \gets \textbf{merge}(df_{label}, df_{\alpha}, on='filename', how='inner')$
    \State $df_{merged} \gets \textbf{merge}(df_{merged}, df_{\beta\_feat}, on='filename', how='inner')$
    \State $df_{final} \gets \textbf{merge}(df_{merged}, df_{\gamma\_feat}, on='filename', how='inner')$
    
    \State \Comment{Simpan hasil akhir}
    \State $\textbf{save\_csv}(df_{final}, File_{HFV})$
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode Integrasi Komponen HFV}
\label{lst:merge_hfv}
\end{listing}

\subsubsection{Implementasi Penggabungan}
Dalam implementasi \texttt{merge\_hfv.py}, \textit{file} label yang dihasilkan oleh \textit{pipeline} $\alpha$ (\texttt{cnn\_payload\_labels.csv}) sengaja dipilih sebagai basis data utama. Ini karena \textit{pipeline} $\alpha$ (ekstraksi \textit{payload}) adalah yang paling ketat; \textit{flow} yang tidak memiliki \textit{payload} (misalnya, \textit{flow} TCP yang kosong) telah dibuang pada fase tersebut.

File komponen $\beta$ dan $\gamma$ mungkin berisi \textit{flow} yang tidak ada di \textit{pipeline} $\alpha$, sehingga mereka juga memuat kolom label mereka sendiri. Untuk mencegah konflik kolom saat penggabungan, kolom-kolom label duplikat ini harus dihapus terlebih dahulu, seperti yang ditunjukkan pada Kode \ref{lst:snippet_merge1}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari merge_hfv.py

    # The delta and gamma files contain redundant label columns.
    # We must drop them before merging to avoid conflicts.
    label_cols_to_drop = ['application', 'category', 'binary_type']

    # Select only filename + delta features
    delta_feature_cols = [col for col in df_delta.columns if col not in label_cols_to_drop]
    df_delta_features_only = df_delta[delta_feature_cols]

    # Select only filename + gamma' features
    gamma_feature_cols = [col for col in df_gamma.columns if col not in label_cols_to_drop]
    df_gamma_features_only = df_gamma[gamma_feature_cols]
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{merge\_hfv.py} menunjukkan penghapusan kolom label duplikat.}
    \label{lst:snippet_merge1}
\end{listing}

Setelah disiapkan, penggabungan akhir dilakukan menggunakan fungsi \texttt{pandas.merge} dengan metode \texttt{how='inner'}. Penggunaan \textit{inner join} (gabung internal) memastikan bahwa hanya \textit{flow} yang \texttt{filename}-nya ada di \textit{semua} komponen (label, $\alpha$, $\beta$, dan $\gamma$) yang akan dimasukkan ke dalam basis data final. Langkah penggabungan krusial ini ditunjukkan pada Kode \ref{lst:snippet_merge2}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari merge_hfv.py

    print("\nMerging... (using 'filename' as the key)")

    # 1. Merge Base Labels + Alpha'' features
    df_merged = pd.merge(df_base_labels, df_alpha_pp, on='filename', how='inner')
    print(f"Shape after merging Alpha'': {df_merged.shape}")

    # 2. Merge with Delta features
    df_merged = pd.merge(df_merged, df_delta_features_only, on='filename', how='inner')
    print(f"Shape after merging Delta:  {df_merged.shape}")

    # 3. Merge with Gamma' features
    df_final = pd.merge(df_merged, df_gamma_features_only, on='filename', how='inner')
    print(f"Shape after merging Gamma': {df_final.shape}")

    # --- Save Final Dataset ---
    print(f"\nAssembly complete. Saving final dataset...")
    df_final.to_csv(OUTPUT_FILE, index=False)
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{merge\_hfv.py} menunjukkan proses \textit{inner merge} sekuensial.}
    \label{lst:snippet_merge2}
\end{listing}

Hasil akhir dari proses ini adalah satu file \texttt{final\_HFV\_dataset.csv}. Basis data fitur inilah yang kemudian digunakan sebagai input untuk melatih dan mengevaluasi berbagai model \textit{machine learning} yang akan dibahas pada bab selanjutnya.