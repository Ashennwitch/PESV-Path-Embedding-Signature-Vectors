%-----------------------------------------------------------------------------%
\chapter{\babTiga}
\label{cha:perancangansistem}

\section{\textit{Overview} Kerangka Kerja}
Secara garis besar, penelitian ini dilakukan dengan alur seperti pada Gambar \ref{fig:macro_arch}.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
\includegraphics[height=0.6\textheight, keepaspectratio]{assets/pics/macro_acrch.png}
\caption{Arsitektur HFV.}
    \label{fig:macro_arch}
\end{figure}

\section{Pemilihan dan Pra-Pemrosesan Dataset ISCX 2016}

\subsection{Deskripsi Dataset ISCX 2016}
Dataset ISCXVPN2016 merupakan dataset yang paling populer dan banyak digunakan dalam penelitian klasifikasi lalu lintas jaringan terenkripsi, khususnya untuk membedakan trafik VPN dan Non-VPN \citep{DraperGil2016, razooqi2025vpn}. Dataset ini berisi trafik nyata dari berbagai aplikasi populer yang dikategorikan ke dalam tiga tugas klasifikasi utama: enkapsulasi (encapsulation), kategori aplikasi, dan aplikasi spesifik. Tugas enkapsulasi membedakan antara trafik VPN dan Non-VPN, kategori aplikasi mengelompokkan trafik berdasarkan jenis layanan seperti chat, email, streaming, transfer file, peer-to-peer, dan VoIP, sedangkan tugas aplikasi menentukan aplikasi spesifik yang menghasilkan trafik tersebut dengan total 16 kelas aplikasi \citep{park2024fast}.

Tabel \ref{tab:dataset_distribution} menunjukkan distribusi kelas pada ketiga tugas klasifikasi dalam dataset ISCXVPN2016.

\begin{table}[h]
    \centering
    \setlength{\tabcolsep}{6pt} % mengurangi jarak antar kolom
    \renewcommand{\arraystretch}{1.2} % sedikit longgar untuk keterbacaan
    \begin{tabularx}{\linewidth}{lX}
        \toprule
        \textbf{Tugas} & \textbf{Kelas} \\
        \midrule
        Enkapsulasi (2 kelas) & VPN, Non-VPN \\
        Kategori (6 kelas) & Chat, Email, Streaming, File Transfer, P2P, VoIP \\
        Aplikasi (16 kelas) & Skype, ICQ, Hangout, Facebook, Email, Gmail, FTP, SCP, SFTP, Netflix, Spotify, Vimeo, YouTube, AIM Chat, VOIPBuster, BitTorrent \\
        \bottomrule
    \end{tabularx}
    \caption{Distribusi Kelas pada Dataset ISCXVPN2016 \citep{park2024fast}}
    \label{tab:dataset_distribution}
\end{table}

Dengan tiga tingkat klasifikasi ini, dataset ISCXVPN2016 menyediakan kerangka kerja evaluasi yang komprehensif dan representatif untuk penelitian network traffic classification, terutama dalam konteks trafik terenkripsi yang semakin berkembang.

\subsection{Pra-Pemrosesan Dataset ISCX 2016}
\label{subsec:prapemrosesan}

Tujuan dari pra-pemrosesan dataset ISCX 2016 adalah untuk mengubah data mentah \texttt{.pcap}, yang berisi data level-paket, menjadi format \textit{flow-level} yang terstruktur dan bersih. Proses ini penting karena dataset aslinya, yang berisi sekitar 310.000 \textit{flow}, mengandung banyak trafik yang tidak relevan atau \textit{noise} yang dihasilkan oleh sistem operasi atau konfigurasi jaringan (seperti DNS, NBSS, LLMNR), bukan oleh aplikasi yang menjadi target analisis.

Proses ini didasarkan pada metodologi yang diuraikan oleh \citep{park2024fast}, yang mengadopsi dan memperluas proses pembersihan dua tahap dari \citep{baek2023preprocessing}. Pra-pemrosesan ini terdiri dari tiga langkah utama:

\subsubsection{Konversi Paket-ke-Flow (menggunakan SplitCap)}
Langkah pertama adalah mengubah file \texttt{.pcap} mentah menjadi \textit{bidirectional flows} (sesi). Konversi ini krusial karena analisis level \textit{flow} (konversasi utuh) memberikan konteks yang jauh lebih kaya daripada analisis level paket (kata-kata individual). Analisis \textit{flow} juga memungkinkan ekstraksi fitur statistik yang lebih bermakna (misalnya, durasi \textit{flow}, total byte, waktu antar-paket) yang esensial untuk membedakan trafik terenkripsi tanpa perlu menginspeksi \textit{payload}.

Untuk tugas ini, \textit{tool} SplitCap digunakan. SplitCap dipilih karena merupakan \textit{tool} khusus yang teroptimasi (ditulis dalam C/C++) untuk menangani file \texttt{.pcap} besar secara efisien dan akurat dalam mengelola sesi, yang esensial untuk mereproduksi metodologi penelitian.

Untuk mengotomatisasi pemrosesan pada seluruh dataset, sebuah skrip Python digunakan untuk memanggil SplitCap pada setiap file \texttt{.pcap}, seperti yang ditunjukkan pada Kode~\ref{lst:splitcap_automation}. Perlu dicatat penambahan argumen \texttt{-p 500} yang penting untuk membatasi sesi paralel dan menghindari galat ``Too many open files''.

\begin{listing}[H]
    \setstretch{1}    
    \begin{algorithmic}[1] % [1] adds line numbers
        \Procedure{RunSplitCap}{$pcap\_file\_path, output\_dir$}
            \State $split\_argument \gets \text{'session'}$ \Comment{Mode bidirectional flows}
            \State $parallel\_limit \gets \text{'500'}$ \Comment{Limit file handles}
            
            \State $command \gets$ list()
            \State $command.append(\text{'mono'})$
            \State $command.append(\text{SPLITCAP\_EXECUTABLE})$
            \State $command.append(\text{'-r'}, pcap\_file\_path)$
            \State $command.append(\text{'-o'}, output\_dir)$
            \State $command.append(\text{'-s'}, split\_argument)$
            \State $command.append(\text{'-p'}, parallel\_limit)$
            
            \State \Call{SystemExecute}{$command$}
            \State \Return \textbf{True}
        \EndProcedure
    \end{algorithmic}
        \caption{Otomasi Command SplitCap} 
    \label{lst:splitcap_automation}
\end{listing}

\subsubsection{Pembersihan Awal (Rule A \& B)}
Setelah semua \texttt{.pcap} dikonversi menjadi \textit{flow} (total $\sim$310.000), proses pembersihan dua tahap yang diadopsi dari \citep{baek2023preprocessing} diterapkan:
\begin{enumerate}
    \item \textbf{Penghapusan Flow Berbasis Protokol (Rule A):} \textit{Flow} yang menggunakan protokol yang tidak relevan dengan aplikasi pengguna, seperti DNS, NBSS, dan LLMNR, dihapus.
    \item \textbf{Verifikasi TCP 3-Way Handshake (Rule B):} \textit{Flow} TCP yang tidak memiliki 3-way handshake (SYN, SYN-ACK, ACK) lengkap di awal sesi akan dibuang. Ini memastikan bahwa hanya sesi TCP yang valid dan lengkap yang disimpan. Implementasi spesifiknya adalah menghapus \textit{flow} TCP dengan lebih dari 4 paket di mana paket pertamanya tidak memiliki \textit{flag} SYN.
\end{enumerate}

Setelah penerapan kedua aturan ini, jumlah \textit{flow} berkurang secara signifikan menjadi \textbf{29.195 \textit{flow}}.

\subsubsection{Penyaringan Tambahan (Rule C)}
Langkah terakhir adalah penyaringan tambahan yang diimplementasikan oleh \citep{park2024fast} untuk menghilangkan \textit{flow} yang dianggap tidak penting untuk tujuan penelitian. Penyaringan ini (disebut Rule C) secara spesifik mencari dan menghapus \textit{flow} yang memenuhi \textit{semua} kriteria berikut:
\begin{itemize}
    \item Protokol adalah \textbf{UDP}.
    \item Alamat IP tujuan adalah \textbf{\texttt{255.255.255.255}} (broadcast).
    \item \textit{Payload} spam berisi string \textbf{\texttt{"Beacon\textasciitilde"}} \citep{park2024fast}.
\end{itemize}

\medskip
\noindent\textbf{Implementasi Penyaringan (Rule A, B, dan C)} \\
Ketiga aturan penyaringan (A, B, dan C) diimplementasikan dalam satu skrip Python menggunakan Scapy untuk membaca dan menganalisis setiap \textit{flow} \texttt{.pcap} satu per satu. Fungsi inti dari proses validasi ini ditunjukkan pada Kode~\ref{lst:scapy_filtering}.

\begin{listing}[H]
    \setstretch{1}    
    \begin{algorithmic}[1] % [1] adds line numbers
        
        \State $DisallowedLayers \gets \{DNS, NTP, DHCP, BOOTP\}$
        \State $DisallowedPorts \gets \{137, 1900, 5353, 5355\}$

        \Procedure{IsFlowValid}{$pcap\_path$}
            \State $packets \gets \Call{ReadPcap}{pcap\_path}$
            \If{$packets$ is empty} \Return \textbf{false} \EndIf
            
            \State $first \gets packets[0]$
            \State \Comment{Rule B: Validasi TCP Handshake (SYN flag)}
            \If{$first$ is TCP \textbf{and} \text{'S'} $\notin$ $first.flags$}
                \State \Return \textbf{false}
            \EndIf

            \State \Comment{Iterasi paket untuk Rule A dan C}
            \For{\textbf{each} $pkt$ \textbf{in} $packets$}
                \State \Comment{Rule A: Cek Layer Terlarang}
                \If{$pkt$ has layer $\in DisallowedLayers$} 
                    \State \Return \textbf{false} 
                \EndIf

                \If{$pkt$ is UDP}
                    \State \Comment{Rule A: Cek Port Terlarang}
                    \If{$pkt.sport \in DisallowedPorts$ \textbf{or} $pkt.dport \in DisallowedPorts$}
                        \State \Return \textbf{false}
                    \EndIf
                    
                    \State \Comment{Rule C: Cek UDP Beacon Broadcast}
                    \If{$pkt.dst == \text{'255.255.255.255'}$ \textbf{and} $\text{"Beacon\textasciitilde"} \in pkt.payload$}
                        \State \Return \textbf{false}
                    \EndIf
                \EndIf
            \EndFor

            \State \Return \textbf{true}
        \EndProcedure

        \Statex % Empty line for separation

        \Procedure{MainFiltering}{}
            \State \Call{CreateDirectory}{$OutputDirectory$}
            
            \For{\textbf{each} $file$ \textbf{in} $InputDirectory$}
                \State $isValid \gets \Call{IsFlowValid}{file}$
                
                \If{$isValid$ \textbf{is true}}
                    \State \Call{CopyFile}{$file, OutputDirectory$}
                \EndIf
            \EndFor
        \EndProcedure

    \end{algorithmic}
        \caption{Penyaringan \textit{flow} menggunakan Scapy untuk menerapkan Rule A, B, dan C.} 
    \label{lst:scapy_filtering}
\end{listing}

Setelah ketiga langkah pra-pemrosesan ini selesai, dataset final yang bersih berisi total \textbf{13.487 \textit{flow}}. Dataset yang telah difilter secara ketat inilah yang kemudian digunakan untuk semua proses ekstraksi fitur dan pelatihan model selanjutnya dalam penelitian ini.

\subsection{Pemilihan \textit{Sample} dari \textit{Flow} yang Telah Di-ekstrak}

Setelah proses pra-pemrosesan di bagian \ref{subsec:prapemrosesan} selesai, didapatkan total \textbf{13.487 \textit{flow}} yang bersih. Analisis awal terhadap 13.487 \textit{flow} ini mengungkapkan adanya ketidakseimbangan kelas (\textit{class imbalance}) yang ekstrem pada dataset. Seperti yang ditunjukkan pada Gambar \ref{fig:full_dist_before}, beberapa aplikasi seperti \textit{Skype} memiliki 4.704 sampel, sementara \textit{ICQ} dan \textit{AIM Chat} hanya memiliki kurang dari 50 sampel. Ketidakseimbangan ini dapat menyebabkan model klasifikasi menjadi bias terhadap kelas mayoritas dan memiliki performa buruk pada kelas minoritas.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/binary_dist.png}
        \caption{Tugas Biner (13.487 flow)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/category_dist.png}
        \caption{Tugas Kategori (13.487 flow)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/application_dist.png}
        \caption{Tugas Aplikasi (13.487 flow)}
    \end{subfigure}

    \caption{Distribusi dataset penuh (13.487 flow) sebelum kurasi. Terlihat jelas adanya ketidakseimbangan kelas yang signifikan.}
    \label{fig:full_dist_before}
\end{figure}

Oleh karena itu, dilakukan langkah kurasi dataset untuk menciptakan dataset yang lebih seimbang untuk eksperimen. Dataset yang dikurasi ini (selanjutnya disebut Dataset v2) dirancang untuk memenuhi dua batasan utama:
\begin{enumerate}
    \item Untuk kesederhanaan, dataset akan berisi 6 kelas aplikasi yang paling representatif.
    \item Untuk menjaga komparabilitas dengan penelitian lain \citep{park2024fast}, dataset harus mencakup \textit{semua} 6 kelas kategori (Chat, Email, File Transfer, P2P, Streaming, VoIP).
\end{enumerate}

Tantangannya adalah, jika hanya mengambil 6 aplikasi dengan jumlah sampel terbesar (misalnya \textit{Skype}, \textit{Email}, \textit{SCP}, \textit{VOIPBuster}, \textit{FTP}, \textit{Facebook}), batasan kedua tidak terpenuhi karena tidak ada perwakilan untuk kategori \textit{Streaming} dan \textit{P2P}.

Solusinya adalah melakukan proses \textit{pick-and-choose} dengan memilih 6 aplikasi yang dapat menjadi ``perwakilan'' dengan jumlah sampel terbesar untuk \textit{setiap} kategori. Aplikasi yang terpilih adalah:
\begin{itemize}
    \item \textbf{Skype} (4.704 flow): Perwakilan terbesar untuk Chat, VoIP, dan File Transfer.
    \item \textbf{Email} (1.901 flow): Perwakilan terbesar untuk Email.
    \item \textbf{SCP} (1.741 flow): Perwakilan terbesar kedua untuk File Transfer.
    \item \textbf{VOIPBuster} (1.196 flow): Perwakilan terbesar untuk VoIP-saja.
    \item \textbf{YouTube} (379 flow): Perwakilan terbesar untuk Streaming.
    \item \textbf{BitTorrent} (363 flow): Satu-satunya perwakilan untuk P2P.
\end{itemize}

Proses \textit{pick-and-choose} ini menghasilkan Dataset v2 dengan total \textbf{10.284 \textit{flow}}. Proses penyaringan ini diimplementasikan dengan skrip Python yang membaca 13.487 \textit{flow} dari direktori sumber dan hanya menyalin \textit{flow} yang termasuk dalam 6 aplikasi target ke direktori tujuan. Logika inti dari skrip penyaringan ditunjukkan pada Kode \ref{lst:filter_v2_pseudo}.

\begin{listing}[H]
    \setstretch{1}    
    \begin{algorithmic}[1] % [1] adds line numbers
        
        \State $TargetApps \gets \{\text{'Skype'}, \text{'Email'}, \text{'SCP'}, \text{'VOIPBuster'}, \text{'YouTube'}, \text{'BitTorrent'}\}$
        
        \State \Comment{Map keyword ke nama aplikasi (Urutan penting: spesifik $\to$ umum)}
        \State $KeywordMap \gets \textbf{OrderedMap}$
        \State $KeywordMap.\text{add}(\text{'facebook\_chat'}, \text{'Facebook'})$
        \State $KeywordMap.\text{add}(\text{'gmail'}, \text{'Gmail'})$
        \State \dots \Comment{Dan seterusnya sesuai definisi Python}
        
        \Statex % Empty line

        \Procedure{FilterDatasetV2}{}
            \State \Call{CreateDirectory}{$DestDir$}
            \State $files \gets \Call{ListFiles}{$SourceDir$}$
            
            \For{\textbf{each} $filename$ \textbf{in} $files$}
                \State $lowerName \gets \Call{ToLower}{$filename$}$
                \State $foundApp \gets \textbf{null}$
                
                \State \Comment{Identifikasi aplikasi berdasarkan keyword}
                \For{$(keyword, appName)$ \textbf{in} $KeywordMap$}
                    \If{$keyword$ \textbf{in} $lowerName$}
                        \State $foundApp \gets appName$
                        \State \textbf{break} \Comment{Berhenti pada kecocokan pertama}
                    \EndIf
                \EndFor
                
                \State \Comment{Salin file jika aplikasi termasuk dalam target}
                \If{$foundApp \neq \textbf{null}$ \textbf{and} $foundApp \in TargetApps$}
                    \State \Call{CopyFilePreserveMetadata}{$filename, SourceDir, DestDir$}
                \EndIf
                
            \EndFor
        \EndProcedure

    \end{algorithmic}
        \caption{Penyaringan 13.487 \textit{flow} bersih menjadi Dataset v2 (10.284 \textit{flow}) berdasarkan 6 aplikasi yang telah ditentukan.} 
    \label{lst:filter_v2_pseudo}
\end{listing}

Setelah proses kurasi ini selesai, distribusi kelas pada Dataset v2 (10.284 \textit{flow}) menjadi lebih seimbang, meskipun masih menunjukkan ketidakseimbangan yang wajar, seperti terlihat pada Gambar \ref{fig:v2_dist_after}. Dataset v2 (VPN/Non-VPN) inilah yang digunakan sebagai \textit{full dataset}, selain \textit{VPN Only} dataset yang akan dijelaskan di bagian berikutnya.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/full-binary_dist.png}
        \caption{Tugas Biner (v2)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/full-category_dist.png}
        \caption{Tugas Kategori (v2)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/full-application_dist.png}
        \caption{Tugas Aplikasi (v2)}
    \end{subfigure}

    \caption{Distribusi Dataset v2 (10.284 flow) setelah proses kurasi.}
    \label{fig:v2_dist_after}
\end{figure}

\subsection{Kurasi Dataset Khusus VPN (VPN-Only)}
Selain Dataset v2 (10.284 \textit{flow}) yang digunakan untuk eksperimen ‘penuh’ (VPN/Non-VPN), penelitian ini juga bertujuan untuk mengevaluasi performa model pada skenario yang lebih spesifik: mengklasifikasikan trafik yang sudah diketahui terenkripsi (VPN).

Tantangan utamanya adalah, jika kita sekadar memfilter Dataset v2 (10.284 \textit{flow}) untuk sampel VPN-nya, dataset yang dihasilkan menjadi tidak representatif dan trivial. Sebagai contoh, aplikasi SCP (1.741 \textit{flow}) akan hilang seluruhnya karena tidak memiliki sampel VPN. Lebih kritis lagi, kategori P2P hanya akan diwakili oleh BitTorrent, dan Email hanya oleh Email. Ini berarti model tidak akan belajar membedakan ‘Chat’ dari ‘P2P’, melainkan hanya membedakan ‘Skype’ dari ‘BitTorrent’, yang merupakan tugas yang jauh lebih mudah.

Untuk mengatasi ini, sebuah dataset ‘Khusus VPN’ yang baru dikurasi langsung dari 13.487 \textit{flow} bersih. Tujuannya adalah untuk menciptakan dataset yang tetap menantang dengan tetap mencakup semua 6 kategori. Dataset ini memiliki batasan yang tidak terhindarkan: kategori P2P dan Email masing-masing hanya akan diwakili oleh BitTorrent (363 \textit{flow} VPN) dan Email (137 \textit{flow} VPN). Oleh karena itu, tantangan sebenarnya dari dataset ini adalah kemampuan model untuk membedakan kategori yang saling tumpang tindih (Chat, VoIP, File Transfer, Streaming) yang diwakili oleh beberapa aplikasi.

Untuk memaksimalkan tumpang tindih ini, 9 aplikasi dipilih:
\begin{itemize}
    \item \textbf{Chat/VoIP/File Transfer (Tumpang Tindih):} Skype (1246 \textit{flow}), Hangout (325 \textit{flow}), Facebook (247 \textit{flow})
    \item \textbf{File Transfer Tambahan:} FTP (101 \textit{flow})
    \item \textbf{Streaming (Variasi):} YouTube (157 \textit{flow}), Spotify (91 \textit{flow}), Netflix (63 \textit{flow})
    \item \textbf{Perwakilan Kategori Wajib:} BitTorrent (363 \textit{flow}), Email (137 \textit{flow})
\end{itemize}

Berbeda dengan Dataset v2 yang disalin secara fisik, dataset ini difilter secara dinamis saat runtime menggunakan skrip klasifikasi utama. Konfigurasi skrip diatur ke \texttt{EXPERIMENT\_MODE = "VPN\_ONLY"} dan \texttt{TARGET\_APPS} diisi dengan 9 aplikasi yang dipilih, seperti ditunjukkan pada Kode \ref{lst:filter_vpnonly_pseudo}.

\begin{listing}[H]
    \setstretch{1}    
    \begin{algorithmic}[1] % [1] adds line numbers
        
        \State $TargetApps \gets \{\text{'Skype'}, \text{'Hangout'}, \text{'Facebook'},$
\Statex \hspace{6.5em} $\text{'BitTorrent'}, \text{'FTP'}, \text{'Email'},$ 
\Statex \hspace{6.5em} $\text{'YouTube'}, \text{'Spotify'}, \text{'Netflix'}\}$
        
        \State \Comment{Map keyword ke nama aplikasi (OrderedMap: Spesifik ke Umum)}
        \State $KeywordMap \gets \textbf{new OrderedMap}$
        \State $KeywordMap.\text{add}(\text{'facebook\_chat'}, \text{'Facebook'})$
        \State $KeywordMap.\text{add}(\text{'hangouts\_video'}, \text{'Hangout'})$
        \State \dots \Comment{Inisialisasi seluruh keyword mapping}
        
        \Statex % Empty line

        \Procedure{FilterVPNOnlyDataset}{}
            \State \Call{CreateDirectory}{$DestDir$}
            \State $files \gets \Call{ListFiles}{$SourceDir$}$
            
            \For{\textbf{each} $filename$ \textbf{in} $files$}
                \If{$filename$ does not end with \text{'.pcap'}}
                    \State \textbf{continue}
                \EndIf

                \State $lowerName \gets \Call{ToLower}{$filename$}$
                
                \State \Comment{Kriteria 1: File harus memiliki prefix 'vpn\_'}
                \If{\textbf{not} $lowerName.\text{startsWith}(\text{'vpn\_'})$}
                    \State \textbf{continue}
                \EndIf

                \State \Comment{Identifikasi aplikasi berdasarkan keyword}
                \State $foundApp \gets \textbf{null}$
                \For{$(keyword, appName)$ \textbf{in} $KeywordMap$}
                    \If{$keyword$ \textbf{in} $lowerName$}
                        \State $foundApp \gets appName$
                        \State \textbf{break} \Comment{Ambil kecocokan pertama}
                    \EndIf
                \EndFor

                \State \Comment{Kriteria 2: Aplikasi harus termasuk dalam 9 target}
                \If{$foundApp \neq \textbf{null}$ \textbf{and} $foundApp \in TargetApps$}
                    \State \Call{CopyFilePreserveMetadata}{$filename, SourceDir, DestDir$}
                \EndIf
                
            \EndFor
        \EndProcedure

    \end{algorithmic}
        \caption{Penyaringan dataset VPN-only menjadi 9 aplikasi representatif, mencakup semua 6 kategori aplikasi utama.} 
    \label{lst:filter_vpnonly_pseudo}
\end{listing}

Proses penyaringan dinamis ini menghasilkan dataset final ‘Khusus VPN’ yang terdiri dari 2.730 \textit{flow} VPN. Distribusi kelas untuk dataset ini ditunjukkan pada Gambar \ref{fig:vpn_only_dist}.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
\begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/vpnonly-category_dist.png}
        \caption{Tugas Kategori (Khusus VPN)}
    \end{subfigure}

    \vspace{0.5em}

    \begin{subfigure}[b]{0.7\textwidth}
        \centering
        \includegraphics[width=\textwidth]{assets/pics/vpnonly-application_dist.png}
        \caption{Tugas Aplikasi (Khusus VPN)}
    \end{subfigure}

    \caption{Distribusi Dataset 'Khusus VPN' (2.730 flow) setelah kurasi dinamis. Tugas Biner secara alami tidak lagi disertakan dalam konteks ini).}
    \label{fig:vpn_only_dist}
\end{figure}

\section{Perancangan dan Implementasi Hybrid Flow Vector (HFV)}

\subsection{Pengantar Kerangka Kerja HFV: $\Omega = (\alpha, \beta, \gamma)$}
\label{subsec:kerangka_hfv}

Klasifikasi lalu lintas jaringan terenkripsi merupakan tantangan fundamental dalam manajemen jaringan modern dan keamanan siber. Teknik tradisional seperti \textit{Deep Packet Inspection} (DPI) menjadi tidak efektif karena enkripsi menyembunyikan konten \textit{payload} \citep{razooqi2025vpn}. Akibatnya, penelitian beralih ke analisis metadata dan karakteristik perilaku \textit{flow} untuk melakukan identifikasi.

Pendekatan yang ada saat ini umumnya terbagi menjadi dua kategori: (1) model berbasis statistik, yang menganalisis fitur-fitur level \textit{flow} seperti ukuran paket dan \textit{Inter-Arrival Time} (IAT) \citep{DraperGil2016}, dan (2) model berbasis \textit{Deep Learning} (DL), yang dapat belajar representasi kompleks langsung dari data mentah, seperti \textit{raw packet payload} \citep{Lotfollahi2019}.

Namun, kedua pendekatan memiliki keterbatasan jika digunakan secara terpisah. Model statistik murni mungkin gagal menangkap pola-pola rumit di level \textit{byte} yang tersembunyi di dalam \textit{payload} terenkripsi. Sebaliknya, model DL yang hanya berfokus pada \textit{payload} mungkin kehilangan konteks perilaku makroskopis dari keseluruhan \textit{flow}.

Untuk mengatasi keterbatasan ini, penelitian ini merancang dan mengimplementasikan sebuah kerangka kerja representasi fitur hibrida, yang disebut \textbf{\textit{Hybrid Flow Vector} (HFV)}. Hipotesis utamanya adalah bahwa dengan menggabungkan dua "pandangan" yang berbeda dari sebuah \textit{flow}---yaitu pola mikroskopis \textit{level-byte} dan perilaku makroskopis \textit{level-flow}---kita dapat menciptakan sebuah "sidik jari" (\textit{fingerprint}) yang jauh lebih kaya dan tangguh untuk klasifikasi. Pendekatan hibrida yang menggabungkan berbagai jenis fitur telah menunjukkan potensi besar dalam penelitian terkait \citep{jorgensen2023extensible}.

Secara formal, HFV ($\Omega$) didefinisikan sebagai gabungan dari tiga komponen vektor yang berbeda:
\begin{equation}
    \Omega = \alpha \oplus \beta \oplus \gamma
\end{equation}
dimana $\oplus$ melambangkan operasi konkatenasi (penggabungan) vektor.

Setiap komponen dirancang untuk menangkap aspek unik dari \textit{flow} jaringan:
\begin{itemize}
    \item \textbf{Komponen $\alpha$ (alpha)}: Sebuah vektor fitur \textit{deep learning} yang diekstraksi dari \textit{raw payload} paket-paket pertama \textit{flow}. Komponen ini mewakili \textbf{pola level-byte} dan dirancang untuk menangkap tanda tangan (\textit{signature}) mikroskopis yang mungkin spesifik untuk aplikasi tertentu, yang diekstraksi menggunakan arsitektur \textit{1D-Convolutional Neural Network} (1D-CNN).

    \item \textbf{Komponen $\beta$ (beta)}: Sebuah vektor fitur statistik yang komprehensif dari \textbf{keseluruhan \textit{flow}} (\textit{flow-level}). Komponen ini mencakup statistik deskriptif (mean, std, min, max, dll.) dari ukuran paket dan IAT untuk seluruh durasi \textit{flow}, yang menggambarkan perilaku makroskopis.

    \item \textbf{Komponen $\gamma$ (gamma)}: Sebuah vektor fitur statistik yang berfokus pada \textbf{level \textit{burst}} komunikasi. Komponen ini dirancang untuk menangkap pola komunikasi \textit{on-off} (kirim-tunggu-kirim) yang sering terjadi pada aplikasi interaktif atau \textit{streaming}.
\end{itemize}

Dengan mengintegrasikan ketiga komponen ini, vektor HFV $\Omega$ menyediakan representasi multi-modal yang holistik dari setiap \textit{flow}, yang menjadi dasar untuk proses klasifikasi \textit{machine learning} yang akan dijelaskan selanjutnya. Bagian-bagian berikut akan merinci perancangan dan implementasi teknis dari ekstraksi masing-masing komponen $\alpha$, $\beta$, dan $\gamma$.

\subsection{Komponen \texorpdfstring{$\alpha$}{alpha}: Ekstraksi Fitur 1D-CNN dari Pola Byte-Level}
\label{subsec:komponen_alpha}

Komponen $\alpha$ dirancang untuk menangkap \textbf{pola mikroskopis level-byte} yang ada di dalam \textit{payload} terenkripsi. Berbeda dengan analisis statistik ($\beta$ dan $\gamma$) yang melihat \textit{flow} dari luar, komponen $\alpha$ mencoba "mengintip" ke dalam data mentah itu sendiri.

Literatur telah menunjukkan bahwa meskipun terenkripsi, \textit{raw packet payload} seringkali masih mengandung pola-pola yang dapat dipelajari oleh model \textit{Deep Learning}, khususnya \textit{1D-Convolutional Neural Networks} (1D-CNN) \citep{wang2017end, Lotfollahi2019}. Pola-pola ini dapat berupa artefak dari protokol aplikasi, \textit{padding}, atau tanda tangan (\textit{signature}) unik lainnya yang tidak dihilangkan oleh enkripsi.

Proses untuk menghasilkan vektor fitur $\alpha$ dibagi menjadi dua fase utama, yang diimplementasikan dalam dua skrip Python terpisah: (1) Ekstraksi Data Payload Mentah, dan (2) Pelatihan Encoder dan Generasi Fitur.

\subsubsection{Fase 1: Ekstraksi Data \textit{Raw Payload}}
\label{ssubsec:alpha_fase1}

Fase pertama, yang diimplementasikan dalam skrip \texttt{alpha\_1.py}, bertanggung jawab untuk mengubah \textit{flow} \texttt{.pcap} bersih (hasil pra-pemrosesan) menjadi format dataset yang siap digunakan oleh CNN. Logika inti dari skrip ini dijelaskan dalam Kode \ref{lst:pseudo_alpha1}.

\begin{listing}[H]
    \setstretch{1}    
    \begin{algorithmic}[1]
        \State $N_{pkt}, L_{pay} \gets 10, 784$ \Comment{Konfigurasi dimensi input CNN}
        
        \Procedure{ProcessFile}{$filepath$}
            \State $fname \gets \Call{Basename}{filepath}$
            \State \Comment{Tentukan Label (App, Category, Binary) dari nama file}
            \State $labels \gets \Call{MapKeywordToLabel}{fname, KeywordMap}$
            
            \State $matrix \gets \Call{Zeros}{N_{pkt}, L_{pay}}$
            \State $packets \gets \Call{ReadPcap}{filepath}$
            \State $count \gets 0$

            \For{\textbf{each} $pkt$ \textbf{in} $packets$}
                \If{$count \geq N_{pkt}$} \textbf{break} \EndIf
                
                \State $payload \gets \Call{GetTCPUDPPayload}{pkt}$
                \If{$payload$ is empty} \textbf{continue} \EndIf

                \State \Comment{Normalisasi byte (0-255) ke float (0.0-1.0) \& potong/padding}
                \State $normData \gets \Call{NormalizeAndPad}{payload, L_{pay}}$
                \State $matrix[count] \gets normData$
                \State $count \gets count + 1$
            \EndFor

            \Return $(matrix, labels)$ \textbf{if} $count > 0$ \textbf{else} \textbf{null}
        \EndProcedure

        \Statex 

        \Procedure{MainExtraction}{}
            \State $files \gets \Call{ListFiles}{SourceDir}$
            \State $dataList, labelList \gets \textbf{new List}, \textbf{new List}$

            \State \Comment{Eksekusi paralel untuk mempercepat proses IO}
            \State $results \gets \Call{ParallelExec}{ProcessFile, files}$

            \For{\textbf{each} $res$ \textbf{in} $results$}
                \If{$res \neq \textbf{null}$}
                    \State $dataList.\text{add}(res.matrix)$
                    \State $labelList.\text{add}(res.labels)$
                \EndIf
            \EndFor

            \State \Comment{Susun Tensor 3D (Samples, 10, 784) dan simpan}
            \State $X \gets \Call{Stack}{dataList}$; $Y \gets \Call{DataFrame}{labelList}$
            \State \Call{SaveNumpy}{X, OutputNpy}; \Call{SaveCSV}{Y, OutputCsv}
        \EndProcedure

    \end{algorithmic}
        \caption{Fase 1: Ekstraksi Data Payload Mentah.} 
    \label{lst:pseudo_alpha1}
\end{listing}

Implementasi spesifik dari langkah ekstraksi dan normalisasi \textit{payload} ditunjukkan pada Kode \ref{lst:snippet_alpha1}. Bagian ini adalah inti dari skrip \texttt{alpha\_1.py}, di mana setiap byte dinormalisasi menjadi nilai antara 0.0 dan 1.0.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari alpha_1.py
# (dalam fungsi process_pcap_for_cnn)

def process_pcap_for_cnn(pcap_filepath):
    try:
        # ... (kode pengambilan label) ...

        # This will be our (10, 784) array for this one flow
        flow_data = np.zeros((N_PACKETS, PAYLOAD_LEN), dtype=np.float32)

        packets = rdpcap(pcap_filepath)
        packet_count = 0

        for pkt in packets:
            if packet_count >= N_PACKETS:
                break

            # Find the payload (bytes *after* TCP/UDP header)
            payload = None
            if TCP in pkt:
                payload = bytes(pkt[TCP].payload)
            elif UDP in pkt:
                payload = bytes(pkt[UDP].payload)
            
            if not payload: # Skip jika tidak ada payload
                continue

            # This is our (784,) vector for this one packet
            normalized_payload = np.zeros(PAYLOAD_LEN, dtype=np.float32)
            copy_len = min(len(payload), PAYLOAD_LEN)

            # Copy data dari byte buffer dan normalisasi (0-255 -> 0.0-1.0)
            byte_data = np.frombuffer(payload[:copy_len], dtype=np.uint8)
            normalized_payload[:copy_len] = byte_data.astype(np.float32) / 255.0

            # Add the packet vector to our flow matrix
            flow_data[packet_count] = normalized_payload
            packet_count += 1
        
        # ... (kode return) ...
    except Exception as e:
        return None
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode yang menunjukkan proses normalisasi payload.}
    \label{lst:snippet_alpha1}
\end{listing}

Hasil akhir dari fase ini adalah dua file:
\begin{itemize}
    \item \texttt{VPNOnly-cnn\_payload\_data.npy}: Sebuah \textit{array} NumPy 3-dimensi dengan bentuk \texttt{(jumlah\_sampel, 10, 784)}.
    \item \texttt{VPNOnly-cnn\_payload\_labels.csv}: File CSV yang berisi label (filename, aplikasi, kategori) yang sesuai untuk setiap sampel.
\end{itemize}

\subsubsection{Fase 2: Pelatihan Encoder \& Generasi Fitur}
\label{ssubsec:alpha_fase2}

Fase kedua, diimplementasikan dalam \texttt{alpha\_2.py}, menggunakan data \texttt{.npy} yang dihasilkan Fase 1 untuk melatih model \textit{Deep Learning} dan mengekstrak fitur $\alpha$. Logika skrip ini diringkas dalam Kode \ref{lst:pseudo_alpha2}.

\begin{listing}[H]
    \setstretch{1}    
    \begin{algorithmic}[1]
        \State $InputShape \gets (10 \times 784, 1)$ \Comment{Input sequence (7840, 1)}
        \State $FeatureDim \gets 128$ \Comment{Dimensi vektor fitur ($alpha''$)}
        
        \Procedure{Build1DCNN}{$numClasses$}
            \State $model \gets \textbf{new SequentialModel}$
            
            \State \Comment{Blok Konvolusi \& Pooling}
            \State $model.\text{add}(\text{Conv1D}(32, \text{kernel}=7, \text{relu}) \to \text{MaxPool}(4))$
            \State $model.\text{add}(\text{Conv1D}(64, \text{kernel}=5, \text{relu}) \to \text{MaxPool}(4))$
            \State $model.\text{add}(\text{Conv1D}(128, \text{kernel}=3, \text{relu}) \to \text{MaxPool}(4))$
            
            \State $model.\text{add}(\text{Flatten})$
            \State \Comment{Layer Encoder (Tempat ekstraksi fitur)}
            \State $model.\text{add}(\text{Dense}(FeatureDim, \text{relu}, name=\text{'encoder'}))$
            \State $model.\text{add}(\text{Dropout}(0.5))$
            
            \State $model.\text{add}(\text{Dense}(numClasses, \text{softmax}))$
            \State $model.\text{compile}(\text{optimizer='adam'}, \text{loss='categorical'})$
            \Return $model$
        \EndProcedure

        \Statex

        \Procedure{TrainAndExtractFeatures}{}
            \State \Comment{Muat data \& Reshape menjadi time-series 1D}
            \State $X \gets \Call{LoadNpy}{DataFile}.\text{reshape}(-1, InputShape)$
            \State $df_{labels} \gets \Call{LoadCSV}{LabelsFile}$
            \State $Y \gets \Call{OneHotEncode}{df_{labels}.\text{application}}$
            
            \State \Comment{Split Data (Train: 80\%, Val: 20\%)}
            \State $X_{tr}, X_{val}, Y_{tr}, Y_{val} \gets \Call{TrainTestSplit}{X, Y, 0.2}$
            
            \State $model \gets \Call{Build1DCNN}{Y.\text{uniqueClasses}}$
            
            \State \Comment{Latih model dengan Early Stopping}
            \State $model.\text{fit}(X_{tr}, Y_{tr}, \text{validation}=(X_{val}, Y_{val}), \text{patience}=10)$
            
            \State \Comment{Buat model baru hanya sampai layer 'encoder'}
            \State $encoder \gets \text{Model}(inputs=model.input, outputs=model.\text{getLayer}(\text{'encoder'}))$
            \State \Call{SaveModel}{encoder, OutputModelFile}
            
            \State \Comment{Ekstrak fitur $\alpha$ dari seluruh dataset}
            \State $features \gets encoder.\text{predict}(X)$ \Comment{Output shape: (N, 128)}
            
            \State \Comment{Gabungkan filename dengan fitur baru dan simpan}
            \State $finalData \gets \Call{Concat}{df_{labels}.\text{filename}, features}$
            \State \Call{SaveCSV}{finalData, OutputAlphaV3File}
        \EndProcedure

    \end{algorithmic}
        \caption{Fase 2: Pelatihan Encoder dan Generasi Fitur $\alpha$.} 
    \label{lst:pseudo_alpha2}
\end{listing}

Seperti dijelaskan dalam pseudocode, sebuah model 1D-CNN dibangun (Kode \ref{lst:snippet_alpha2}). Tujuan utamanya adalah untuk melatih lapisan \textit{Dense} (neuron=128) agar menjadi ekstraktor fitur yang baik. Lapisan ini diberi nama "encoder\_output" agar dapat diekstraksi dengan mudah setelah pelatihan.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari alpha_2.py
# (Fungsi build_model)

def build_model(num_classes):
    print("Building 1D-CNN model...")

    input_layer = Input(shape=INPUT_SHAPE) # INPUT_SHAPE = (7840, 1)

    # Convolutional Block 1
    x = Conv1D(filters=32, kernel_size=7, activation='relu', padding='same')(input_layer)
    x = MaxPooling1D(pool_size=4)(x)

    # Convolutional Block 2
    x = Conv1D(filters=64, kernel_size=5, activation='relu', padding='same')(x)
    x = MaxPooling1D(pool_size=4)(x)

    # Convolutional Block 3
    x = Conv1D(filters=128, kernel_size=3, activation='relu', padding='same')(x)
    x = MaxPooling1D(pool_size=4)(x)

    x = Flatten()(x)

    # --- This is our Feature Vector ---
    # We give it a name so we can easily extract it later
    x = Dense(FEATURE_VECTOR_SIZE, activation='relu', name="encoder_output")(x)
    x = Dropout(0.5)(x)
    # ----------------------------------

    # Output classifier layer (Hanya untuk pelatihan)
    output_layer = Dense(num_classes, activation='softmax', name="classifier_output")(x)

    # Create the full model
    model = Model(inputs=input_layer, outputs=output_layer)
    
    # ... (kode compile) ...
    return model
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{alpha\_2.py} menunjukkan arsitektur 1D-CNN.}
    \label{lst:snippet_alpha2}
\end{listing}

Model \textit{lengkap} dilatih pada 80\% data untuk mengklasifikasikan \textbf{aplikasi}. Dengan "memaksa" model membedakan 'Skype' dari 'Netflix', lapisan "encoder\_output" (Dense 128) belajar menghasilkan representasi vektor 128-dimensi yang informatif.

Setelah pelatihan selesai, bagian \textit{classifier} dibuang, dan model \textit{encoder} yang sudah "pintar" ini digunakan untuk memprediksi \textit{seluruh} dataset. Proses ekstraksi ini ditunjukkan pada Kode \ref{lst:snippet_alpha3}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari alpha_2.py
# (dari fungsi main)

    # ... (Kode memuat data dan melatih 'model') ...

    print("Extracting and saving the encoder model...")

    # 4. Buat model encoder baru dengan "memotong" model yang sudah dilatih
    encoder_model = Model(
        inputs=model.input,
        outputs=model.get_layer("encoder_output").output
    )

    encoder_model.save(OUTPUT_ENCODER_MODEL_FILE)

    # 5. Gunakan encoder untuk mengekstrak fitur dari *seluruh* dataset
    print(f"Generating {FEATURE_VECTOR_SIZE}-dim features for all {X_full.shape[0]} samples...")
    alpha_prime_prime_features = encoder_model.predict(X_full, batch_size=128)

    # ... (Kode menyimpan 'alpha_prime_prime_features' ke CSV) ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode yang menunjukkan proses ekstraksi fitur $\alpha$.}
    \label{lst:snippet_alpha3}
\end{listing}

Hasil dari Fase 2 adalah \textit{array} 2D \texttt{(jumlah\_sampel, 128)}. Ini adalah \textbf{komponen fitur $\alpha$} final, yang kemudian disimpan ke file CSV untuk digabungkan dengan komponen $\beta$ dan $\gamma$. Ilsutrasi dari proses yang dilakukan pada tahap ini terlihat pada Gambar \ref{fig:alpha_extraction_vertical}

\begin{figure}[H]
    \centering
    \begin{tikzpicture}[
        % Use 'positioning' library logic
        node distance=0.8cm and 1cm, 
        % Global styles
        font=\small,
        >=Stealth,
        % Node Styles
        block/.style = {
            rectangle, draw, thick, fill=blue!10,
            text width=11em, text centered, rounded corners,
            minimum height=3em, drop shadow
        },
        data_node/.style = {
            cylinder, draw, thick, fill=orange!10,
            shape aspect=0.25, minimum height=3em, text centered,
            text width=10em, drop shadow
        },
        label_node/.style = {
            cylinder, draw, thick, fill=green!10, shape aspect=0.25,
            minimum height=2.5em, text centered, text width=9em, 
            drop shadow
        },
        note/.style = {
            text width=12em, align=center, font=\scriptsize, color=black!80
        },
        arrow/.style = {
            thick, ->
        }
    ]
        
        % --- 1. START: PCAP ---
        \node (pcap) [data_node] {Flow .pcap Bersih\\ \scriptsize (dari Pra-Pemrosesan)};
        
        % --- 2. STEP 1 ---
        \node (step1) [block, below=of pcap] {\textbf{Fase 1: Ekstraksi Payload} \\ \texttt{(...1.py)}};
        \node (step1_note) [note, below=0.0cm of step1] {10 pkt $\times$ 784 byte.\\Normalisasi 0.0-1.0.};
        
        % --- 3. RAW PAYLOAD ---
        \node (npy) [data_node, below=of step1_note] {Data Payload Mentah};
        \node (npy_note) [note, below=0.0cm of npy] {Bentuk: (N, 7840, 1)};

        % --- 4. MODEL CONTAINER (Outer Box) ---
        % FIX: Increased height to 8cm so the Softmax layer stays strictly inside
        \node (model) [block, fill=red!5, below=1.2cm of npy_note, text width=14em, minimum height=8cm] {};
        
        % Title
        \node [below=0.2cm, align=center] at (model.north) {\textbf{Fase 2: Arsitektur 1D-CNN} \\ \texttt{(...2.py)}};

        % --- INTERNAL LAYERS ---
        % Adjusted spacing so they flow nicely inside the taller box
        \node (conv) [rectangle, draw, fill=white, text width=10em, text centered, minimum height=2em, drop shadow, below=1.5cm of model.north] {Blok Konvolusi\\(Conv/Pool)};
        
        \node (encoder) [rectangle, draw, fill=yellow!20, text width=10em, text centered, minimum height=2em, drop shadow, below=0.8cm of conv] {Lapisan Encoder\\(Dense 128)};
        
        \node (classifier) [rectangle, draw, fill=white, text width=10em, text centered, minimum height=2em, drop shadow, below=0.8cm of encoder] {Lapisan Klasifikasi\\(Softmax)};
        
        % --- 5. SIDE INPUT (LABELS) ---
        % FIX: Moved 'right=4.5cm' to create a long arrow, solving the text overlap
        \node (labels) [label_node, right=4.5cm of model.north east, anchor=center, yshift=-2.5cm] {Label Aplikasi};
        \node (labels_note) [note, below=0.1cm of labels] {(Mis: 'Skype')};

        % --- 6. OUTPUT ---
        % Moved alpha down significantly to allow room for the long arrow text
        \node (alpha) [data_node, fill=yellow!20, below=2cm of model] {Komponen Fitur $\alpha$};
        \node (alpha_note) [note, below=0.0cm of alpha] {Bentuk: (N, 128)};

        % --- ARROWS ---
        \draw [arrow] (pcap) -- (step1);
        \draw [arrow] (step1) -- (step1_note) -- (npy);
        \draw [arrow] (npy_note) -- node[left, font=\scriptsize, align=right] {(Data Latih)} (model);
        
        % FIX: Arrow goes straight horizontal to the model edge
        \draw [arrow] (labels.west) -- node[above, font=\scriptsize] {(Target)} (labels.west -| model.east);

        % Output arrow
        \draw [arrow, line width=1.5pt] (model.south) -- node[midway, right, align=left, font=\scriptsize, text width=15em, xshift=0.2cm] {Generasi Fitur \texttt{.predict()} dari \textbf{encoder}} (alpha);

    \end{tikzpicture}
    \caption{Diagram alir proses ekstraksi komponen fitur $\alpha$ (alpha). Data payload mentah diekstrak dari \texttt{.pcap} (Fase 1), kemudian sebuah 1D-CNN dilatih untuk mengklasifikasikan aplikasi. Lapisan \textit{encoder} (Dense 128) dari model yang telah dilatih kemudian digunakan untuk menghasilkan vektor fitur $\alpha$ akhir.}
    \label{fig:alpha_extraction_vertical}
\end{figure}

\subsection{Komponen \texorpdfstring{$\beta$}{beta}: Statistik Level Flow}
\label{subsec:komponen_beta}

Komponen $\beta$ dirancang untuk menangkap \textbf{pola makroskopis} dari keseluruhan \textit{flow} jaringan. Komponen ini didasarkan pada hipotesis bahwa bahkan ketika \textit{payload} dienkripsi, karakteristik perilaku statistik dari aliran---seperti seberapa sering, seberapa besar, dan kapan paket dikirim---dapat mengungkapkan informasi penting tentang aplikasi yang mendasarinya \citep{DraperGil2016}.

Implementasi ekstraksi fitur $\beta$ dilakukan oleh skrip \texttt{v2\_beta.py}. Logika inti dari skrip ini diringkas dalam Pseudocode \ref{lst:pseudo_beta}.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\begin{algorithmic}[1]
    \State \textbf{Input:} FLOW\_DIR (direktori .pcap)
    \State \textbf{Output:} OUTPUT\_CSV (file .csv berisi fitur beta)
    \item[]
    \Function{ProcessPcap}{file}
        \State $(app, cat, type) \gets \Call{GetFlowLabels}{file}$
        \If{$app = null$} \Return $null$ \EndIf
        \State $packets \gets \Call{BacaPcap}{file}$
        \State $clientIP \gets \Call{GetSourceIP}{packets[0]}$
        \State $(sizesC2S, timesC2S) \gets \Call{GetPackets}{packets, clientIP, arahC2S}$
        \State $(sizesS2C, timesS2C) \gets \Call{GetPackets}{packets, clientIP, arahS2C}$
        \State $iatsC2S \gets \Call{HitungSelisih}{timesC2S}$
        \State $iatsS2C \gets \Call{HitungSelisih}{timesS2C}$
        \State $fitur \gets \{\}$
        \State $stats1 \gets \Call{CalcStats}{sizesC2S}$
        \State $stats2 \gets \Call{CalcStats}{sizesS2C}$
        \State $stats3 \gets \Call{CalcStats}{iatsC2S}$
        \State $stats4 \gets \Call{CalcStats}{iatsS2C}$
        \State $stats5 \gets \Call{HitungFiturFlow}{timesC2S, timesS2C}$
        \State $fitur \gets \Call{Gabung}{stats1, stats2, stats3, stats4, stats5}$
        \State \Return $(app, cat, type, fitur)$
    \EndFunction
    \item[]
    \Procedure{Main}{}
        \State $files \gets \Call{GetAllPcap}{FLOW\_DIR}$
        \State $hasil \gets \Call{JalankanParalel}{ProcessPcap, files}$
        \State $df \gets \Call{ToDataFrame}{hasil}$
        \State \Call{SimpanCsv}{df, outputFile}
    \EndProcedure
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode untuk Ekstraksi Komponen $\beta$ (Statistik Level Flow).}
\label{lst:pseudo_beta}
\end{listing}

Kunci dari komponen $\beta$ adalah analisis \textbf{bidireksional}. Untuk setiap \textit{flow}, skrip terlebih dahulu mengidentifikasi IP klien (diasumsikan sebagai IP sumber dari paket pertama), seperti terlihat pada Kode \ref{lst:snippet_beta1}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari v2_beta.py
# (dalam fungsi process_pcap_file)

    # ... (kode inisialisasi list) ...
    try:
        packets = rdpcap(filepath)

        client_ip = None
        for pkt in packets:
            if IP in pkt:
                client_ip = pkt[IP].src
                break

        if client_ip is None:
            return None # Skip non-IP flows
        # ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{v2\_beta.py} menunjukkan identifikasi IP klien.}
    \label{lst:snippet_beta1}
\end{listing}

Setelah IP klien diketahui, skrip melakukan iterasi kedua melalui semua paket untuk memisahkan ukuran dan waktu kedatangan paket ke dalam empat daftar terpisah:
\begin{itemize}
    \item \texttt{c2s\_sizes}: Ukuran paket dari Klien ke Server.
    \item \texttt{c2s\_times}: Waktu kedatangan paket dari Klien ke Server.
    \item \texttt{s2c\_sizes}: Ukuran paket dari Server ke Klien.
    \item \texttt{s2c\_times}: Waktu kedatangan paket dari Server ke Klien.
\end{itemize}

Selanjutnya, waktu kedatangan (misalnya \texttt{c2s\_times}) digunakan untuk menghitung \textit{Inter-Arrival Times} (IAT) untuk setiap arah. Kode \ref{lst:snippet_beta2} menunjukkan bagaimana daftar IAT ini dihitung menggunakan fungsi \texttt{np.diff}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari v2_beta.py
# (dalam fungsi process_pcap_file)

    # ... (kode ekstraksi paket bidireksional) ...

    # 5. Calculate Inter-Arrival Times (IATs)
    # np.diff computes the difference between consecutive elements
    c2s_iats = np.diff(c2s_times).tolist()
    s2c_iats = np.diff(s2c_times).tolist()

    # 6. Calculate all statistical features
    features = {}
    # ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{v2\_beta.py} menunjukkan kalkulasi IAT bidireksional.}
    \label{lst:snippet_beta2}
\end{listing}

Terakhir, fungsi utilitas \texttt{calculate\_stats} (ditampilkan dalam Kode \ref{lst:snippet_beta3}) dipanggil pada keempat daftar ini (\texttt{c2s\_sizes}, \texttt{s2c\_sizes}, \texttt{c2s\_iats}, \texttt{s2c\_iats}).

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari v2_beta.py
# (Fungsi calculate_stats)

def calculate_stats(data_list, prefix):
    """
    Calculates a full statistical profile for a list of numbers.
    Returns a dictionary of features.
    """
    stats = {}

    # Ensure all keys exist, even if they are 0
    stat_names = ['count', 'sum', 'mean', 'std', 'min', 'max', 'median', 'p25', 'p75']
    for name in stat_names:
        stats[f"{prefix}_{name}"] = 0.0

    if not data_list:
        return stats # Return all zeros

    arr = np.array(data_list)

    stats[f"{prefix}_count"] = float(arr.size)
    stats[f"{prefix}_sum"] = float(np.sum(arr))
    stats[f"{prefix}_mean"] = float(np.mean(arr))
    stats[f"{prefix}_min"] = float(np.min(arr))
    stats[f"{prefix}_max"] = float(np.max(arr))
    stats[f"{prefix}_median"] = float(np.median(arr))

    # Percentiles
    stats[f"{prefix}_p25"] = float(np.percentile(arr, 25))
    stats[f"{prefix}_p75"] = float(np.percentile(arr, 75))

    # Std deviation requires at least 2 samples
    if arr.size > 1:
        stats[f"{prefix}_std"] = float(np.std(arr))

    return stats
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{v2\_beta.py} menunjukkan fungsi utilitas kalkulasi 9 statistik.}
    \label{lst:snippet_beta3}
\end{listing}

Fungsi ini menghitung 9 statistik deskriptif: \textit{count} (jumlah), \textit{sum} (total), \textit{mean} (rata-rata), \textit{std} (standar deviasi), \textit{min}, \textit{max}, \textit{median}, \textit{p25} (kuartil 1), dan \textit{p75} (kuartil 3).

Karena 9 statistik ini dihitung untuk 4 daftar yang berbeda (ukuran c2s, ukuran s2c, IAT c2s, IAT s2c), total $9 \times 4 = 36$ fitur statistik bidireksional dihasilkan. Ditambah dengan 3 fitur level \textit{flow} total (durasi, total paket, total volume), komponen $\beta$ akhir terdiri dari \textbf{39 fitur} statistik.

\subsection{Komponen \texorpdfstring{$\gamma$}{gamma}: Statistik Level Burst}
\label{subsec:komponen_gamma}

Komponen $\gamma$ (gamma) dirancang untuk menangkap \textbf{pola komunikasi \textit{on-off}} dari sebuah aliran. Analisis ini berbeda dari komponen $\beta$, yang melihat aliran sebagai satu kesatuan statistik, dan komponen $\alpha$, yang melihat \textit{payload} mentah. Komponen $\gamma$ berfokus pada ritme komunikasi.

Banyak aplikasi, terutama yang bersifat interaktif (seperti \textit{chat} atau VoIP) atau \textit{streaming} (seperti video), tidak mengirimkan data dalam aliran yang konstan. Sebaliknya, mereka mengirimkan sekelompok paket (\textit{burst}) data, diikuti dengan jeda (waktu diam), lalu diikuti dengan \textit{burst} data berikutnya. Menganalisis karakteristik dari \textit{burst} dan jeda ini dapat memberikan sidik jari yang kuat untuk mengidentifikasi jenis aplikasi..

\subsubsection{Definisi Burst}
Kunci dari analisis ini adalah definisi "burst". Dalam penelitian ini, sebuah \textit{burst} didefinisikan sebagai sekelompok paket (satu atau lebih) di mana waktu jeda antar paket tersebut \textbf{kurang dari 1.0 detik}. Jika jeda antar dua paket melebihi ambang batas ini, \textit{burst} sebelumnya dianggap selesai dan \textit{burst} baru dimulai. Nilai ambang batas 1.0 detik ini diatur dalam variabel \texttt{BURST\_IDLE\_THRESHOLD}.

\subsubsection{Implementasi Ekstraksi (gamma.py)}
Ekstraksi fitur $\gamma$ diimplementasikan dalam skrip \texttt{gamma.py}. Berbeda dengan komponen $\beta$, analisis \textit{burst} ini bersifat \textbf{unidireksional} dalam arti menggabungkan paket dari kedua arah (Klien $\leftrightarrow$ Server) menjadi satu urutan waktu sebelum diproses.

Logika inti dari skrip ini diringkas dalam Kode \ref{lst:pseudo_gamma}.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\begin{algorithmic}[1]
    \State \textbf{Definisi:} THRESHOLD $\gets 1.0$ (detik)
    \State \textbf{Input:} file (.pcap)
    \State \textbf{Output:} fitur\_gamma (37 fitur statistik burst)
    \item[]
    \Function{DeteksiBurst}{file}
        \State $packets \gets \Call{GetAllPacketsSorted}{file}$
        \State $(counts, volumes, durations, idles) \gets ([], [], [], [])$
        \State $(firstTime, firstSize) \gets packets[0]$
        \State $(burstCount, burstVolume, burstStart, lastTime) \gets (1, firstSize, firstTime, firstTime)$
        \For{each $(t, s)$ in $packets[1...]$}
            \State $idle \gets t - lastTime$
            \If{$idle < THRESHOLD$}
                \State $burstCount \gets burstCount + 1$
                \State $burstVolume \gets burstVolume + s$
            \Else
                \State \Call{Append}{durations, $lastTime - burstStart$}
                \State \Call{Append}{counts, burstCount}
                \State \Call{Append}{volumes, burstVolume}
                \State \Call{Append}{idles, idle}
                \State $(burstCount, burstVolume, burstStart) \gets (1, s, t)$
            \EndIf
            \State $lastTime \gets t$
        \EndFor
        \State \Call{Append}{durations, $lastTime - burstStart$}
        \State \Call{Append}{counts, burstCount}
        \State \Call{Append}{volumes, burstVolume}
        \State $fitur \gets \{\}$
        \State $fitur[\text{totalBurst}] \gets \Call{Length}{counts}$
        \State $s1 \gets \Call{CalcStats}{counts}$
        \State $s2 \gets \Call{CalcStats}{volumes}$
        \State $s3 \gets \Call{CalcStats}{durations}$
        \State $s4 \gets \Call{CalcStats}{idles}$
        \State $fitur \gets \Call{Gabung}{fitur, s1, s2, s3, s4}$
        \State \Return $fitur$
    \EndFunction
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode untuk Ekstraksi Komponen $\gamma$ (Statistik Level Burst).}
\label{lst:pseudo_gamma}
\end{listing}

Kode \ref{lst:snippet_gamma1} menunjukkan logika inti dari algoritma deteksi \textit{burst} ini. Skrip mengiterasi setiap paket (mulai dari paket kedua) dan menghitung \texttt{idle\_time} (waktu jeda) dari paket sebelumnya.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari gamma.py
# (dalam fungsi process_pcap_file)

    # ... (kode inisialisasi burst pertama) ...
    last_packet_time = all_packets[0][0]

    # Iterate from the *second* packet onwards
    for (pkt_time, pkt_size) in all_packets[1:]:
        idle_time = pkt_time - last_packet_time

        if idle_time < BURST_IDLE_THRESHOLD:
            # --- This packet is PART of the current burst ---
            current_burst_packets += 1
            current_burst_volume += pkt_size
        else:
            # --- This packet is the START of a new burst ---
            # 1. Save the previous burst
            burst_duration = last_packet_time - current_burst_start_time
            burst_packet_counts.append(current_burst_packets)
            burst_volumes.append(current_burst_volume)
            burst_durations.append(burst_duration)

            # 2. Save the idle time that just ended
            burst_idle_times.append(idle_time)

            # 3. Reset for the new burst
            current_burst_packets = 1
            current_burst_volume = pkt_size
            current_burst_start_time = pkt_time

        # Update the time of the last packet seen
        last_packet_time = pkt_time

    # 6. Save the *final* burst after the loop
    burst_duration = last_packet_time - current_burst_start_time
    # ... (append burst terakhir ke list) ...
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{gamma.py} menunjukkan logika deteksi batas burst.}
    \label{lst:snippet_gamma1}
\end{listing}

Jika \texttt{idle\_time} kurang dari ambang batas (1.0 detik), paket saat ini ditambahkan ke \textit{burst} yang sedang berjalan. Jika tidak, \textit{burst} sebelumnya disimpan, dan \texttt{idle\_time} dicatat sebagai waktu jeda antar-\textit{burst}.

Setelah semua paket diproses, skrip memanggil fungsi \texttt{calculate\_stats} (yang sama dengan yang digunakan pada komponen $\beta$) pada empat daftar yang dihasilkan:
\begin{itemize}
    \item \texttt{burst\_packet\_counts}: Daftar jumlah paket untuk setiap burst.
    \item \texttt{burst\_volumes}: Daftar total volume (byte) untuk setiap burst.
    \item \texttt{burst\_durations}: Daftar durasi (detik) untuk setiap burst.
    \item \texttt{burst\_idle\_times}: Daftar waktu jeda (detik) antar burst.
\end{itemize}

Menghitung 9 statistik (count, mean, std, dll.) untuk 4 daftar ini menghasilkan $9 \times 4 = 36$ fitur. Ditambah dengan satu fitur tambahan, \texttt{total\_burst\_count}, komponen $\gamma$ akhir terdiri dari \textbf{37 fitur} statistik level \textit{burst}.

\subsection{Integrasi Komponen HFV dan Pembuatan Basis Data Fitur}
\label{subsec:integrasi_hfv}

Setelah ketiga komponen fitur diekstraksi secara terpisah dalam tiga \textit{pipeline} yang independen, langkah terakhir adalah mengintegrasikannya menjadi satu vektor fitur hibrida (HFV) tunggal. Proses ini diimplementasikan dalam skrip \texttt{merge\_hfv.py}.

Tujuan dari skrip ini adalah untuk membuat satu basis data (file \texttt{.csv}) di mana setiap baris mewakili satu \textit{flow} yang unik, dan kolom-kolomnya berisi:
\begin{enumerate}
    \item Informasi label (misalnya, \texttt{filename}, \texttt{application}, \texttt{category}).
    \item 128 fitur dari komponen $\alpha$ (pola \textit{byte} 1D-CNN).
    \item 39 fitur dari komponen $\beta$ (statistik level \textit{flow}).
    \item 37 fitur dari komponen $\gamma$ (statistik level \textit{burst}).
\end{enumerate}

Penggabungan ini menghasilkan vektor HFV $\Omega$ final dengan total $128 + 39 + 37 = \mathbf{204}$ fitur untuk setiap \textit{flow}.

Proses penggabungan ini diringkas dalam Kode \ref{lst:merge_hfv}. Kunci dari proses ini adalah penggunaan \texttt{filename} sebagai \textit{primary key} unik untuk menggabungkan semua \textit{DataFrame} komponen.

\begin{listing}[H]
\setstretch{1}
\begin{algorithm}[H]
\caption{Pseudocode Integrasi Komponen HFV}
\begin{algorithmic}[1]
    \State \textbf{Input:}
    \State \quad $File_{label} \gets$ \texttt{VPNOnly-cnn\_payload\_labels.csv}
    \State \quad $File_{\alpha} \gets$ \texttt{VPNOnly-alpha\_double\_prime\_component\_v3.csv}
    \State \quad $File_{\beta} \gets$ \texttt{VPNOnly-delta\_component\_v2.csv}
    \State \quad $File_{\gamma} \gets$ \texttt{VPNOnly-gamma\_prime\_component\_v2.csv}
    \State \textbf{Output:} $File_{HFV} \gets$ \texttt{VPNOnly-final\_PESV\_dataset\_v3.csv}
    
    \State \Comment{Muat semua komponen ke dalam DataFrame pandas}
    \State $df_{label} \gets \textbf{load\_csv}(File_{label})$ \Comment{Basis label (9,720 baris)}
    \State $df_{\alpha} \gets \textbf{load\_csv}(File_{\alpha})$ \Comment{Fitur Alpha (9,720 baris)}
    \State $df_{\beta} \gets \textbf{load\_csv}(File_{\beta})$ \Comment{Fitur Beta (~10k baris)}
    \State $df_{\gamma} \gets \textbf{load\_csv}(File_{\gamma})$ \Comment{Fitur Gamma (~10k baris)}
    
    \State \Comment{Persiapan merge: Hapus kolom label duplikat dari Beta dan Gamma}
    \State $cols\_to\_drop \gets$ ['application', 'category', 'binary\_type']
    \State $df_{\beta\_feat} \gets \textbf{drop\_columns}(df_{\beta}, cols\_to\_drop)$
    \State $df_{\gamma\_feat} \gets \textbf{drop\_columns}(df_{\gamma}, cols\_to\_drop)$
    
    \State \Comment{Lakukan penggabungan inner-join sekuensial}
    \State $df_{merged} \gets \textbf{merge}(df_{label}, df_{\alpha}, on='filename', how='inner')$
    \State $df_{merged} \gets \textbf{merge}(df_{merged}, df_{\beta\_feat}, on='filename', how='inner')$
    \State $df_{final} \gets \textbf{merge}(df_{merged}, df_{\gamma\_feat}, on='filename', how='inner')$
    
    \State \Comment{Simpan hasil akhir}
    \State $\textbf{save\_csv}(df_{final}, File_{HFV})$
\end{algorithmic}
\end{algorithm}
\captionsetup{font=small}
\caption{Pseudocode Integrasi Komponen HFV}
\label{lst:merge_hfv}
\end{listing}

\subsubsection{Implementasi Penggabungan}
Dalam implementasi \texttt{merge\_hfv.py}, \textit{file} label yang dihasilkan oleh \textit{pipeline} $\alpha$ (\texttt{cnn\_payload\_labels.csv}) sengaja dipilih sebagai basis data utama. Ini karena \textit{pipeline} $\alpha$ (ekstraksi \textit{payload}) adalah yang paling ketat; \textit{flow} yang tidak memiliki \textit{payload} (misalnya, \textit{flow} TCP yang kosong) telah dibuang pada fase tersebut.

File komponen $\beta$ dan $\gamma$ mungkin berisi \textit{flow} yang tidak ada di \textit{pipeline} $\alpha$, sehingga mereka juga memuat kolom label mereka sendiri. Untuk mencegah konflik kolom saat penggabungan, kolom-kolom label duplikat ini harus dihapus terlebih dahulu, seperti yang ditunjukkan pada Kode \ref{lst:snippet_merge1}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari merge_hfv.py

    # The delta and gamma files contain redundant label columns.
    # We must drop them before merging to avoid conflicts.
    label_cols_to_drop = ['application', 'category', 'binary_type']

    # Select only filename + delta features
    delta_feature_cols = [col for col in df_delta.columns if col not in label_cols_to_drop]
    df_delta_features_only = df_delta[delta_feature_cols]

    # Select only filename + gamma' features
    gamma_feature_cols = [col for col in df_gamma.columns if col not in label_cols_to_drop]
    df_gamma_features_only = df_gamma[gamma_feature_cols]
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{merge\_hfv.py} menunjukkan penghapusan kolom label duplikat.}
    \label{lst:snippet_merge1}
\end{listing}

Setelah disiapkan, penggabungan akhir dilakukan menggunakan fungsi \texttt{pandas.merge} dengan metode \texttt{how='inner'}. Penggunaan \textit{inner join} (gabung internal) memastikan bahwa hanya \textit{flow} yang \texttt{filename}-nya ada di \textit{semua} komponen (label, $\alpha$, $\beta$, dan $\gamma$) yang akan dimasukkan ke dalam basis data final. Langkah penggabungan krusial ini ditunjukkan pada Kode \ref{lst:snippet_merge2}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari merge_hfv.py

    print("\nMerging... (using 'filename' as the key)")

    # 1. Merge Base Labels + Alpha'' features
    df_merged = pd.merge(df_base_labels, df_alpha_pp, on='filename', how='inner')
    print(f"Shape after merging Alpha'': {df_merged.shape}")

    # 2. Merge with Delta features
    df_merged = pd.merge(df_merged, df_delta_features_only, on='filename', how='inner')
    print(f"Shape after merging Delta:  {df_merged.shape}")

    # 3. Merge with Gamma' features
    df_final = pd.merge(df_merged, df_gamma_features_only, on='filename', how='inner')
    print(f"Shape after merging Gamma': {df_final.shape}")

    # --- Save Final Dataset ---
    print(f"\nAssembly complete. Saving final dataset...")
    df_final.to_csv(OUTPUT_FILE, index=False)
    \end{minted}
    \captionsetup{font=small}
    \caption{Potongan kode dari \texttt{merge\_hfv.py} menunjukkan proses \textit{inner merge} sekuensial.}
    \label{lst:snippet_merge2}
\end{listing}

Hasil akhir dari proses ini adalah satu file \texttt{final\_HFV\_dataset.csv}. Basis data fitur inilah yang kemudian digunakan sebagai input untuk melatih dan mengevaluasi berbagai model \textit{machine learning} yang akan dibahas pada bagian selanjutnya.

\subsection{Arsitektur Model Machine Learning untuk Klasifikasi}
\label{subsec:arsitektur_model}

Bagian terakhir dari sistem yang diusulkan adalah arsitektur klasifikasi yang bertanggung jawab untuk memetakan vektor fitur HFV ke kelas target yang sesuai (VPN/Non-VPN, Kategori, atau Aplikasi). Berdasarkan eksperimen pendahuluan dan studi literatur, penelitian ini mengadopsi pendekatan \textit{ensemble learning} dengan metode \textit{Soft Voting}, yang menggabungkan kekuatan algoritma \textbf{Random Forest (RF)} dan \textbf{XGBoost}.

Implementasi arsitektur ini terdapat dalam skrip \texttt{hfv\_final\_classifier\_soft\_voting\_best\_params.py}.

\subsubsection{Pipeline Klasifikasi}
Proses klasifikasi dirancang sebagai sebuah \textit{pipeline} sistematis yang terdiri dari beberapa tahap:

\begin{enumerate}
    \item \textbf{Pra-pemrosesan Data:}
    \begin{itemize}
        \item \textbf{Pembersihan Nilai:} Menangani nilai \texttt{NaN} (kosong) atau \texttt{Infinity} yang mungkin muncul dari perhitungan statistik (misalnya pembagian dengan nol) dengan menggantinya menjadi nol.
        \item \textbf{Encoding Label:} Mengubah label target string (misalnya "Chat", "Streaming") menjadi representasi numerik menggunakan \texttt{LabelEncoder} agar dapat diproses oleh algoritma XGBoost.
        \item \textbf{Pemisahan Data:} Membagi dataset menjadi set pelatihan (80\%) dan set pengujian (20\%) secara \textit{stratified} untuk menjaga proporsi kelas yang seimbang.
    \end{itemize}
    
    \item \textbf{Standarisasi Fitur:}
    Semua fitur dinormalisasi menggunakan \texttt{StandardScaler} untuk mengubah distribusi fitur sehingga memiliki rata-rata 0 dan standar deviasi 1. Langkah ini krusial, terutama untuk model yang sensitif terhadap skala fitur, dan merupakan praktik terbaik dalam \textit{pipeline} machine learning \citep{scikitlearn}.
    
    \item \textbf{Model Ensemble (Soft Voting):}
    Inti dari arsitektur ini adalah \textit{Voting Classifier} dengan strategi \textit{'soft voting'}. Strategi ini tidak hanya menghitung suara mayoritas (hard voting), tetapi merata-ratakan probabilitas prediksi dari setiap model dasar. Hal ini memungkinkan model yang lebih "yakin" dengan prediksinya memiliki bobot lebih besar dalam keputusan akhir.
\end{enumerate}

Diagram alir proses klasifikasi ini diilustrasikan pada Gambar \ref{fig:classification_pipeline}.

\begin{figure}[H]
    \centering
    \captionsetup{font=small}
\includegraphics[width=\textwidth]{assets/pics/soft_voting_arch.png}
\caption{Arsitektur pipeline klasifikasi HFV menggunakan ensemble Soft Voting.}
    \label{fig:classification_pipeline}
\end{figure}

\subsubsection{Konfigurasi Model dan Hyperparameter}
Pemilihan model \textbf{Random Forest} dan \textbf{XGBoost} didasarkan pada performa superior mereka dalam menangani data tabular berdimensi tinggi pada penelitian sebelumnya.

Untuk mendapatkan performa optimal, parameter terbaik (\textit{best parameters}) untuk setiap model dan setiap tugas klasifikasi (Binary, Category, Application) telah ditentukan sebelumnya melalui proses \textit{Hyperparameter Tuning} yang ekstensif (menggunakan \textit{RandomizedSearchCV}). Nilai-nilai parameter ini "di-hardcode" ke dalam skrip final untuk memastikan reproduktifitas dan efisiensi eksekusi.

Konfigurasi parameter terbaik yang digunakan untuk eksperimen mode \texttt{FULL} dirangkum dalam Tabel \ref{tab:best_params}.

\begin{table}[H]
    \centering
    \caption{Parameter Terbaik untuk Ensemble Soft Voting (Mode FULL)}
    \label{tab:best_params}
    \resizebox{\textwidth}{!}{%
    \begin{tabular}{@{}llll@{}}
        \toprule
        \textbf{Tugas} & \textbf{Model} & \textbf{Parameter Utama} & \textbf{Catatan} \\ 
        \midrule
        \textbf{Binary} & Random Forest & n\_est=300, depth=20, crit=entropy & Model Utama \\
        (VPN/Non-VPN) & XGBoost & n\_est=200, lr=0.1, depth=6 & Model Pendukung \\
        \midrule
        \textbf{Category} & XGBoost & n\_est=100, lr=0.2, depth=10 & Model Utama \\
        (6 Kelas) & Random Forest & n\_est=200, depth=20, crit=gini & Model Pendukung \\
        \midrule
        \textbf{Application} & Random Forest & n\_est=100, depth=None, crit=gini & Model Utama \\
        (6 Kelas) & XGBoost & n\_est=200, lr=0.1, depth=6 & Model Pendukung \\
        \bottomrule
    \end{tabular}%
    }
\end{table}

Implementasi konfigurasi ini dalam kode Python ditunjukkan pada Kode \ref{lst:snippet_model_params}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari hfv_final_classifier_soft_voting_best_params.py

HARDCODED_PARAMS = {
    "FULL": {
        "binary_type": {
            # Winner: Random Forest
            "rf": {"n_estimators": 300, "min_samples_split": 10, "max_depth": 20, "criterion": "entropy", "class_weight": "balanced"},
            # Partner: XGBoost
            "xgb": {"n_estimators": 200, "learning_rate": 0.1, "max_depth": 6, "subsample": 0.8, "eval_metric": "mlogloss"}
        },
        "category": {
            # Winner: XGBoost
            "xgb": {"n_estimators": 100, "learning_rate": 0.2, "max_depth": 10, "subsample": 1.0, "eval_metric": "mlogloss"},
            # ... (RF params)
        },
        # ... (Application params)
    },
    # ... (VPN_ONLY params)
    \end{minted}
    \captionsetup{font=small}
    \caption{Definisi parameter hardcoded untuk setiap tugas klasifikasi.}
    \label{lst:snippet_model_params}
\end{listing}

\subsubsection{Algoritma Eksekusi Klasifikasi}
Logika eksekusi utama dari sistem klasifikasi ini, mulai dari pembuatan pipeline hingga evaluasi, dijelaskan dalam Pseudocode \ref{lst:alg_classifier}.

\begin{listing}[H]
    \setstretch{1}    
    \begin{algorithmic}[1]
        \Require Dataset HFV (.csv), Mode Eksperimen (FULL/VPN\_ONLY)
        \Ensure Laporan Klasifikasi (Akurasi, F1-Score)

        \Procedure{GetVotingPipeline}{$task, mode$}
            \State $params \gets HardcodedParams[mode][task]$
            
            \State \Comment{1. Instansiasi Model Dasar}
            \State $rf \gets \textbf{new } RandomForest(params[\text{'rf'}])$
            \State $xgb \gets \textbf{new } XGBoost(params[\text{'xgb'}])$

            \State \Comment{2. Buat Ensemble Voting (Soft/Probabilitas)}
            \State $voting \gets \textbf{new } VotingClassifier(\{rf, xgb\}, \text{'soft'})$

            \State \Comment{3. Bungkus dalam Pipeline dengan Scaler}
            \State \Return $\textbf{new } Pipeline(\text{StandardScaler}, voting)$
        \EndProcedure

        \Statex

        \Procedure{RunTask}{$df, task$}
            \State \Comment{Persiapan Data \& Encoding}
            \State $X \gets \Call{CleanValues}{df[HFV\_Features]}$
            \State $y \gets \Call{LabelEncode}{df[task]}$

            \State \Comment{Split Data 80:20 (Stratified)}
            \State $X_{tr}, X_{te}, y_{tr}, y_{te} \gets \Call{TrainTestSplit}{X, y, 0.2}$

            \State \Comment{Dapatkan Pipeline \& Latih}
            \State $model \gets \Call{GetVotingPipeline}{task, ExperimentMode}$
            \State $model.\text{fit}(X_{tr}, y_{tr})$

            \State \Comment{Evaluasi Hasil}
            \State $preds \gets model.\text{predict}(X_{te})$
            \State $metrics \gets \Call{CalculateMetrics}{y_{te}, preds}$
            \State \Call{PrintReport}{metrics}
        \EndProcedure

        \Statex

        \Procedure{MainClassification}{}
            \State $df \gets \Call{LoadData}{FinalPESVFile}$
            \State $Tasks \gets [\text{'binary\_type'}, \text{'category'}, \text{'application'}]$

            \For{\textbf{each} $task$ \textbf{in} $Tasks$}
                \If{$task \in df.\text{columns}$}
                    \State \Call{RunTask}{df, task}
                \EndIf
            \EndFor
        \EndProcedure

    \end{algorithmic}
        \caption{Alur kerja klasifikasi menggunakan ensemble Soft Voting.} 
    \label{lst:alg_classifierg}
\end{listing}

Implementasi teknis dari pembuatan \textit{ensemble} ini menggunakan pustaka \texttt{scikit-learn} ditunjukkan pada Potongan Kode \ref{lst:snippet_voting}.

\begin{listing}[H]
\setstretch{1}
    \begin{minted}[
        fontsize=\footnotesize
    ]{python}
# Potongan kode dari hfv_final_classifier_soft_voting_best_params.py
# (Fungsi get_voting_pipeline)

    # 1. Instantiate Individual Models
    rf_clf = RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1, **params['rf'])
    xgb_clf = XGBClassifier(random_state=RANDOM_STATE, n_jobs=-1, **params['xgb'])

    # 2. Create Voting Ensemble (Soft Voting)
    # Soft voting averages the probabilities, which is usually superior for diverse models
    voting_clf = VotingClassifier(
        estimators=[('rf', rf_clf), ('xgb', xgb_clf)],
        voting='soft',
        n_jobs=-1
    )

    # 3. Wrap in Pipeline
    pipeline = Pipeline([
        ('scaler', StandardScaler()),
        ('voting', voting_clf)
    ])
    
    return pipeline
    \end{minted}
    \captionsetup{font=small}
    \caption{Implementasi \textit{VotingClassifier} dengan strategi \textit{soft voting}.}
    \label{lst:snippet_voting}
\end{listing}

Dengan arsitektur ini, sistem HFV dapat secara otomatis beradaptasi dan memberikan prediksi terbaik untuk berbagai tingkat granularitas klasifikasi (biner, kategori, atau aplikasi) dengan memanfaatkan kekuatan gabungan dari dua algoritma \textit{state-of-the-art}.
